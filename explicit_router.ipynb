{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjdInLhcWv252IeXap6MoS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbaremoney/goldenTicket/blob/main/explicit_router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist --q\n",
        "!pip install -U torchvision --q\n",
        "!pip install sympy==1.13.3 --q # was getting weird dependency error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu4ID2PSPpKS",
        "outputId": "89c8b4fc-c46d-4a94-a20e-577e6fd71675"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lTW6kW-NPSp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d7415b-dbd0-4d75-98cf-54a6dbd4c01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import v2\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import random\n",
        "import torch.autograd as autograd\n",
        "import math\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import combinations_with_replacement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "FOWDRguTPhAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a397a1-d0f6-4cf8-c463-27628074d361"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 21:53:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPyXZetfQcnq",
        "outputId": "263bd220-b30a-4e1d-b4c4-103d7c5d745c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTrainingDataLoaders(dataset_name, download=True, BATCH_SIZE=128):\n",
        "    \"\"\"\n",
        "    Handling data preprocessing & loading.\n",
        "\n",
        "    Args:\n",
        "      dataset_name (str): name of dataset to be loaded\n",
        "      download (bool): Whether or not you'll download the dataset locally\n",
        "      BATCH_SIZE (int): batch size to be used during training\n",
        "\n",
        "    Returns:\n",
        "      info (dict): dictionary directly from medmnist.INFO containing metadata\n",
        "      task (str): string indicating type of task ie 'binary-class', 'multi-class'\n",
        "      n_classes (int): int indicating number of classes in dataset\n",
        "      train_loader (DataLoader): provides iterator over training dataset, provides batches\n",
        "      train_loader_at_eval (DataLoader): evaluation version of train_loader, double batch size\n",
        "      test_loader (DataLoader): test version of train_loader, similar to train_loader_at_eval\n",
        "\n",
        "    \"\"\"\n",
        "    data_flag = dataset_name\n",
        "\n",
        "    info = INFO[data_flag]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # RGBtransform = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
        "\n",
        "    # preprocessing\n",
        "    if n_channels == 3:\n",
        "        data_transform = v2.Compose([\n",
        "            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "            v2.Normalize(mean=[.5], std=[.5])\n",
        "        ])\n",
        "\n",
        "    if n_channels == 1:\n",
        "        data_transform = v2.Compose([\n",
        "            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "            v2.RGB(),\n",
        "            v2.Normalize(mean=[.5], std=[.5])\n",
        "        ])\n",
        "\n",
        "    # load the data\n",
        "    train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "    test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "    pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "    # encapsulate data into dataloader form\n",
        "    train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "    return info, task, n_classes, train_loader, train_loader_at_eval, test_loader"
      ],
      "metadata": {
        "id": "6Odkl4L8l4Bw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Dataset, Sampler\n",
        "from torchvision.transforms import v2\n",
        "import random\n",
        "import math\n",
        "\n",
        "# --- your PaddedMedMNIST as-is ---\n",
        "TARGET_LENGTH = 14\n",
        "\n",
        "class PaddedMedMNIST(Dataset):\n",
        "    def __init__(self, dataset, target_length=TARGET_LENGTH):\n",
        "        self.dataset = dataset\n",
        "        self.target_length = target_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        padded = torch.zeros(self.target_length, dtype=torch.float32)\n",
        "        padded[:label.numel()] = label\n",
        "        return img, padded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "# --- Balanced batch sampler ---\n",
        "class BalancedBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Yields batches with (approximately) equal counts from each sub-dataset inside a ConcatDataset.\n",
        "\n",
        "    Args:\n",
        "        concat_ds: ConcatDataset([...])\n",
        "        batch_size: total batch size\n",
        "        strategy: 'upsample' (with replacement so every batch is balanced across entire epoch)\n",
        "                  or 'min' (stop when any dataset can't fill its share; no replacement)\n",
        "        drop_last: drop final incomplete batch (recommended True)\n",
        "        generator: optional torch.Generator for reproducibility\n",
        "    \"\"\"\n",
        "    def __init__(self, concat_ds: ConcatDataset, batch_size: int,\n",
        "                 strategy: str = \"upsample\", drop_last: bool = True,\n",
        "                 generator: torch.Generator | None = None):\n",
        "        assert isinstance(concat_ds, ConcatDataset), \"BalancedBatchSampler requires a ConcatDataset\"\n",
        "        self.concat_ds = concat_ds\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.K = len(concat_ds.datasets)\n",
        "        assert self.K >= 2, \"Need at least 2 datasets to balance.\"\n",
        "        assert self.batch_size >= self.K, \"batch_size must be >= number of datasets\"\n",
        "\n",
        "        # Per-dataset shares sum to batch_size (distribute any remainder to the first few datasets).\n",
        "        base = self.batch_size // self.K\n",
        "        extra = self.batch_size % self.K\n",
        "        self.shares = [base + (1 if i < extra else 0) for i in range(self.K)]\n",
        "\n",
        "        self.lengths = [len(ds) for ds in concat_ds.datasets]\n",
        "        self.offsets = []\n",
        "        running = 0\n",
        "        for L in self.lengths:\n",
        "            self.offsets.append(running)\n",
        "            running += L\n",
        "\n",
        "        self.strategy = strategy\n",
        "        self.drop_last = drop_last\n",
        "        self.gen = generator\n",
        "\n",
        "        # Precompute how many *balanced* batches we can make without replacement (for 'min').\n",
        "        self.max_full_batches_min = min(\n",
        "            (L // s) if s > 0 else 0\n",
        "            for L, s in zip(self.lengths, self.shares)\n",
        "        )\n",
        "\n",
        "        # For 'upsample', define epoch length as number of batches ≈ total_len / batch_size\n",
        "        total_len = sum(self.lengths)\n",
        "        self.num_batches_upsample = max(1, total_len // self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.strategy == \"min\":\n",
        "            return self.max_full_batches_min\n",
        "        else:  # 'upsample'\n",
        "            return self.num_batches_upsample\n",
        "\n",
        "    def _randperm(self, n):\n",
        "        if self.gen is None:\n",
        "            return torch.randperm(n)\n",
        "        return torch.randperm(n, generator=self.gen)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Build per-dataset index pools (local indices)\n",
        "        pools = []\n",
        "        for k, L in enumerate(self.lengths):\n",
        "            order = self._randperm(L).tolist()\n",
        "            pools.append(order)\n",
        "\n",
        "        if self.strategy == \"min\":\n",
        "            num_batches = self.max_full_batches_min\n",
        "            for _ in range(num_batches):\n",
        "                batch = []\n",
        "                for k in range(self.K):\n",
        "                    take = self.shares[k]\n",
        "                    # pop 'take' elements from the pool (no replacement)\n",
        "                    chosen_local = pools[k][:take]\n",
        "                    pools[k] = pools[k][take:]\n",
        "                    # map to global indices via offset\n",
        "                    off = self.offsets[k]\n",
        "                    batch.extend([off + i for i in chosen_local])\n",
        "                yield batch\n",
        "            # If not dropping last and any remainder exists (rare with this scheme), we could add a small final batch.\n",
        "            # But by design with 'min' we usually keep batches uniform and drop incomplete ones.\n",
        "            if not self.drop_last:\n",
        "                # Attempt to form one last (possibly smaller) balanced-ish batch\n",
        "                leftovers = []\n",
        "                for k in range(self.K):\n",
        "                    take = min(self.shares[k], len(pools[k]))\n",
        "                    off = self.offsets[k]\n",
        "                    leftovers.extend([off + i for i in pools[k][:take]])\n",
        "                if len(leftovers) > 0:\n",
        "                    yield leftovers\n",
        "\n",
        "        else:  # 'upsample' with replacement from small datasets\n",
        "            num_batches = self.num_batches_upsample\n",
        "            for _ in range(num_batches):\n",
        "                batch = []\n",
        "                for k in range(self.K):\n",
        "                    take = self.shares[k]\n",
        "                    off = self.offsets[k]\n",
        "                    pool = pools[k]\n",
        "                    if len(pool) < take:\n",
        "                        # Re-shuffle / top-up this pool\n",
        "                        pool.extend(self._randperm(self.lengths[k]).tolist())\n",
        "                    chosen_local = pool[:take]\n",
        "                    del pool[:take]\n",
        "                    batch.extend([off + i for i in chosen_local])\n",
        "                yield batch\n",
        "\n",
        "\n",
        "# --- Your loader builder, swapped to use the BalancedBatchSampler ---\n",
        "def get_combined_medmnist_loader(dataset_names, batch_size=128, download=True, train=True,\n",
        "                                 strategy: str = \"upsample\", drop_last: bool = True,\n",
        "                                 num_workers: int = 4, pin_memory: bool = True):\n",
        "    all_datasets = []\n",
        "\n",
        "    for name in dataset_names:\n",
        "        info = INFO[name]\n",
        "        DataClass = getattr(medmnist, info['python_class'])\n",
        "        n_channels = info['n_channels']\n",
        "\n",
        "        if n_channels == 3:\n",
        "            transform = v2.Compose([\n",
        "                v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "            ])\n",
        "        else:\n",
        "            transform = v2.Compose([\n",
        "                v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.RGB(),  # force 3 channels\n",
        "                v2.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "            ])\n",
        "\n",
        "        split = 'train' if train else 'test'\n",
        "        raw_dataset = DataClass(split=split, transform=transform, download=download)\n",
        "        padded_dataset = PaddedMedMNIST(raw_dataset)\n",
        "        all_datasets.append(padded_dataset)\n",
        "\n",
        "    combined_dataset = ConcatDataset(all_datasets)\n",
        "    batch_sampler = BalancedBatchSampler(\n",
        "        combined_dataset, batch_size=batch_size, strategy=strategy, drop_last=drop_last\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: when using batch_sampler, do not also pass batch_size/shuffle/sampler\n",
        "    loader = DataLoader(\n",
        "        combined_dataset,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    return loader"
      ],
      "metadata": {
        "id": "Bonoxbvkl66l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a training function that returns a list of the losses during training.\n",
        "def trainit(model,\n",
        "            NUM_EPOCHS,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            task,\n",
        "            n_classes,\n",
        "            return_losses=False,\n",
        "            no_progress=False):\n",
        "  # define loss function\n",
        "  if task == \"multi-label, binary-class\":\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "  else:\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "  if return_losses:\n",
        "      losses = []\n",
        "  # iterate over epochs for training run\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      model.train()\n",
        "      if no_progress:\n",
        "        loader = train_loader\n",
        "      else:\n",
        "        loader=tqdm(train_loader)\n",
        "      for inputs, targets in loader:\n",
        "          inputs  = inputs.to(device, non_blocking=True)\n",
        "          targets = targets.to(device, non_blocking=True)\n",
        "          # forward + backward + optimize\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)[:,0:n_classes]\n",
        "          if task == 'multi-label, binary-class':\n",
        "              targets = targets.to(torch.float32)\n",
        "              loss = criterion(outputs, targets)\n",
        "          else:\n",
        "              targets = targets.squeeze(1)\n",
        "              loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if return_losses:\n",
        "              losses.append(loss.item())\n",
        "  if return_losses:\n",
        "      return losses\n",
        "\n",
        "# Define an evaluation function\n",
        "def test(split,\n",
        "         model,\n",
        "         train_loader_at_eval,\n",
        "         test_loader,\n",
        "         task,\n",
        "         n_classes,\n",
        "         data_flag,\n",
        "         return_metrics=False):\n",
        "    # define loss function\n",
        "    if task == \"multi-label, binary-class\":\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    y_true = torch.tensor([])\n",
        "    y_score = torch.tensor([])\n",
        "\n",
        "    data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            # inputs  = inputs.to(device, non_blocking=True)\n",
        "            # targets = targets.to(device, non_blocking=True)\n",
        "            outputs = model(inputs)[:,0:n_classes]\n",
        "\n",
        "            if task == 'multi-label, binary-class':\n",
        "                targets = targets.to(torch.float32)\n",
        "                outputs = outputs.softmax(dim=-1)\n",
        "            else:\n",
        "                targets = targets.squeeze(1)\n",
        "                outputs = outputs.softmax(dim=-1)\n",
        "                targets = targets.float().resize_(len(targets), 1)\n",
        "\n",
        "            y_true = torch.cat((y_true, targets), 0)\n",
        "            y_score = torch.cat((y_score, outputs), 0)\n",
        "\n",
        "        y_true = y_true.numpy()\n",
        "        y_score = y_score.detach().numpy()\n",
        "\n",
        "        evaluator = Evaluator(data_flag, split)\n",
        "        metrics = evaluator.evaluate(y_score)\n",
        "\n",
        "        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
        "\n",
        "        if return_metrics:\n",
        "          return metrics"
      ],
      "metadata": {
        "id": "oUaM9_BmmTe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def signed_kaiming_constant_(tensor, a=0, mode='fan_in', nonlinearity='relu', k=0.5, sparsity=0):\n",
        "\n",
        "    fan = nn.init._calculate_correct_fan(tensor, mode)  # calculating correct fan, depends on shape and type of nn\n",
        "    gain = nn.init.calculate_gain(nonlinearity, a)\n",
        "    std = (gain / math.sqrt(fan))\n",
        "    # scale by (1/sqrt(k))\n",
        "    if k != 0:\n",
        "        std *= (1 / math.sqrt(k))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tensor.uniform_(-std, std)\n",
        "        if sparsity > 0:\n",
        "            mask = (torch.rand_like(tensor) > sparsity).float()  # Keeps (1 - sparsity)% weights\n",
        "\n",
        "            tensor *= mask\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GetSubnet(autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "\n",
        "        # Get the subnetwork by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort() # idx will be the indices in sorted order\n",
        "        j = int((1-k) * scores.numel())  # how many we're getting rid of\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten() # flattened scores\n",
        "\n",
        "        # idx[:j] is arr of indices corresponding to j lowest popup scores\n",
        "        flat_out[idx[:j]] = 0 # vectorized assigning them to 0\n",
        "        flat_out[idx[j:]] = 1 # vectorized assigning to 1\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "\n",
        "        # send the gradient g straight-through on the backward pass.\n",
        "        return grad, None\n",
        "\n",
        "\n",
        "class LinearSubnetDynamicMask(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, k=0.5, init=signed_kaiming_constant_, **kwargs):\n",
        "        super().__init__(in_features, out_features, bias if isinstance(bias, bool) else True, **kwargs)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        # Initialize weights\n",
        "        if init == signed_kaiming_constant_:\n",
        "            init(self.weight, k=k)\n",
        "        else:\n",
        "            init(self.weight)\n",
        "\n",
        "        self.weight.requires_grad_(False)\n",
        "        if self.bias is not None:\n",
        "            self.bias.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x, popups, popups_extra, bias_popups, bias_popups_extra):\n",
        "        adj = GetSubnet.apply(\n",
        "            torch.cat((popups.abs(),popups_extra.abs()),dim=-1), self.k\n",
        "        )[:, :self.weight.shape[-1]]\n",
        "        bias_adj = GetSubnet.apply(\n",
        "            torch.cat((bias_popups.abs(),\n",
        "                       bias_popups_extra.abs()), dim=-1), self.k\n",
        "        )[:self.bias.shape[-1]]\n",
        "\n",
        "        w = self.weight * adj\n",
        "        b = self.bias * bias_adj\n",
        "        return F.linear(x, w, b)\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, layer_sizes, k=0.5, init=signed_kaiming_constant_, popups={}):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, (in_f, out_f) in enumerate(layer_sizes):\n",
        "            self.layers.append(LinearSubnet(in_f, out_f,k=k,init=init))\n",
        "            if i < len(layer_sizes) - 1:\n",
        "                self.layers.append(nn.ReLU())\n",
        "\n",
        "    def apply_mask(self, scores,k):\n",
        "      \"\"\"\n",
        "      scores must be same length as number of layers,\n",
        "      each scores matrix must be same shape as respective layer\n",
        "      \"\"\"\n",
        "      for i in range(len(scores)):\n",
        "        self.linear_relu_stack[i*2] = self.linear_relu_stack[i*2] * GetSubnet.forward(scores[i], k)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[1:] != (3, 28, 28):\n",
        "          print(x.shape)\n",
        "          x.unsqueeze_(0)\n",
        "          x = x.repeat(3, 1, 1)\n",
        "        x = self.flatten(x)\n",
        "        for layer in self.layers:\n",
        "                x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e6eHbMc9gWXD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def get_bin_mask(ctx, scores, k):\n",
        "\n",
        "        # Get the subnetwork by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort()\n",
        "        j = int((1-k) * scores.numel())\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten()\n",
        "        flat_out[idx[:j]] = 0\n",
        "        flat_out[idx[j:]] = 1\n",
        "\n",
        "        return out # returns binary mask\n",
        "\n",
        "class ClassicNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            *[z for l in layer_sizes\n",
        "              for z in [nn.Linear(l[0], l[1]), nn.ReLU()]][:-1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def apply_mask(self, whole_mask):\n",
        "      \"\"\"\n",
        "      whole mask must be same length as number of linear layers,\n",
        "      each mask matrix must be same shape as respective layer\n",
        "\n",
        "      assumes ReLu is every other in list\n",
        "      \"\"\"\n",
        "      linear_layers = [layer for layer in self.linear_relu_stack if isinstance(layer, nn.Linear)]\n",
        "\n",
        "      if len(linear_layers) != len(whole_mask):\n",
        "          raise ValueError(f\"Expected {len(linear_layers)} masks, got {len(whole_mask)}\")\n",
        "\n",
        "      for layer, mask in zip(linear_layers, whole_mask):\n",
        "          # Apply mask to weights (in-place)\n",
        "          layer.weight.data *= mask\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "\n",
        "    def clone(self):\n",
        "      return copy.deepcopy(self)\n",
        "\n",
        "\n",
        "# Set up signed Kaiming initialization.\n",
        "def signed_kaiming_constant_(tensor, a=0, mode='fan_in', nonlinearity='relu', k=0.5, sparsity=0):\n",
        "\n",
        "    fan = nn.init._calculate_correct_fan(tensor, mode)  # calculating correct fan, depends on shape and type of nn\n",
        "    gain = nn.init.calculate_gain(nonlinearity, a)\n",
        "    std = (gain / math.sqrt(fan))\n",
        "    # scale by (1/sqrt(k))\n",
        "    if k != 0:\n",
        "        std *= (1 / math.sqrt(k))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tensor.uniform_(-std, std)\n",
        "        if sparsity > 0:\n",
        "            mask = (torch.rand_like(tensor) > sparsity).float()  # Keeps (1 - sparsity)% weights\n",
        "\n",
        "            tensor *= mask\n",
        "\n",
        "        return tensor\n"
      ],
      "metadata": {
        "id": "htWCjCxq1rHY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just doing real numbers\n",
        "in_dim = 1\n",
        "out_dim = 1\n",
        "w = 28\n",
        "d = 10\n",
        "\n",
        "\n",
        "classic_layer_sizes = [[in_dim, w]]\n",
        "for _ in range(d):\n",
        "    classic_layer_sizes.append([w, w])\n",
        "classic_layer_sizes.append([w, out_dim])\n",
        "\n",
        "main_net = ClassicNetwork(classic_layer_sizes)\n",
        "print(main_net)\n",
        "\n",
        "num_points = 10 # number of points for context\n",
        "\n",
        "# generating 100 whole masks to mask entire network\n",
        "whole_masks_list = []\n",
        "\n",
        "for i in range(100):\n",
        "  whole_mask = [] # list of masks for each layer\n",
        "\n",
        "  for layer in main_net.linear_relu_stack:\n",
        "    if isinstance(layer, nn.Linear):\n",
        "\n",
        "      p = random.random()  # random probability in [0, 1]\n",
        "\n",
        "      layer_mask = (torch.rand_like(layer.weight) > p).float()\n",
        "      whole_mask.append(layer_mask)\n",
        "\n",
        "  whole_masks_list.append(whole_mask)\n",
        "\n",
        "# for each whole mask, generate 10 input output pairs ie (network * mask)(input) = output\n",
        "# let input be a integer\n",
        "whole_masks_context_list = []\n",
        "for mask in whole_masks_list:\n",
        "  #inputs = [torch.randn(main_net.input_dim) for _ in range(10)]# generates random tensor of input shape\n",
        "  # use batching instead\n",
        "  inputs = torch.randn(num_points, in_dim) # random tensor size [batch_size, input_dim]\n",
        "  # each row is an input vector, ie with input dim 1 we have column vector\n",
        "\n",
        "  # generate masked network just once\n",
        "  masked_net = main_net.clone().apply_mask(mask)\n",
        "\n",
        "\n",
        "\n",
        "  # generate outputs\n",
        "  outputs = masked_net(inputs) # runs batch\n",
        "\n",
        "  io_list = list(zip(inputs.detach(), outputs.detach())) # x,y pairs\n",
        "  whole_masks_context_list.append(io_list)\n",
        "print(whole_masks_context_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jAUXu-H7wE_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afe8351-130a-49d6-992c-15dbd184683c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassicNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=1, out_features=28, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (15): ReLU()\n",
            "    (16): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (17): ReLU()\n",
            "    (18): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (19): ReLU()\n",
            "    (20): Linear(in_features=28, out_features=28, bias=True)\n",
            "    (21): ReLU()\n",
            "    (22): Linear(in_features=28, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "[[(tensor([-0.1589]), tensor([0.0911])), (tensor([-1.8219]), tensor([0.0911])), (tensor([0.9176]), tensor([0.0911])), (tensor([-0.3174]), tensor([0.0911])), (tensor([0.0772]), tensor([0.0911])), (tensor([1.5879]), tensor([0.0911])), (tensor([0.0347]), tensor([0.0911])), (tensor([0.6774]), tensor([0.0911])), (tensor([-0.3319]), tensor([0.0911])), (tensor([2.0134]), tensor([0.0911]))], [(tensor([0.1815]), tensor([0.1235])), (tensor([-1.4175]), tensor([0.1235])), (tensor([0.4602]), tensor([0.1235])), (tensor([-2.0951]), tensor([0.1235])), (tensor([1.1751]), tensor([0.1235])), (tensor([0.7389]), tensor([0.1235])), (tensor([0.1005]), tensor([0.1235])), (tensor([0.9139]), tensor([0.1235])), (tensor([0.1667]), tensor([0.1235])), (tensor([0.6655]), tensor([0.1235]))], [(tensor([0.5444]), tensor([0.0634])), (tensor([-0.3648]), tensor([0.0634])), (tensor([0.1937]), tensor([0.0634])), (tensor([0.1725]), tensor([0.0634])), (tensor([0.3115]), tensor([0.0634])), (tensor([-0.2265]), tensor([0.0634])), (tensor([-1.5003]), tensor([0.0634])), (tensor([0.5010]), tensor([0.0634])), (tensor([0.8823]), tensor([0.0634])), (tensor([1.5086]), tensor([0.0634]))], [(tensor([-1.3234]), tensor([0.0685])), (tensor([-0.7790]), tensor([0.0685])), (tensor([2.6751]), tensor([0.0685])), (tensor([0.0961]), tensor([0.0685])), (tensor([0.7835]), tensor([0.0685])), (tensor([0.6140]), tensor([0.0685])), (tensor([0.7490]), tensor([0.0685])), (tensor([0.3745]), tensor([0.0685])), (tensor([-1.4189]), tensor([0.0685])), (tensor([0.8427]), tensor([0.0685]))], [(tensor([0.6131]), tensor([0.0732])), (tensor([-0.0172]), tensor([0.0732])), (tensor([0.3057]), tensor([0.0732])), (tensor([0.8939]), tensor([0.0732])), (tensor([-0.3765]), tensor([0.0732])), (tensor([0.0570]), tensor([0.0732])), (tensor([0.1223]), tensor([0.0732])), (tensor([-0.9210]), tensor([0.0732])), (tensor([-0.8104]), tensor([0.0732])), (tensor([-0.8906]), tensor([0.0732]))], [(tensor([0.4580]), tensor([0.1157])), (tensor([0.4919]), tensor([0.1157])), (tensor([-1.6342]), tensor([0.1157])), (tensor([-0.2046]), tensor([0.1157])), (tensor([0.1812]), tensor([0.1157])), (tensor([0.5255]), tensor([0.1157])), (tensor([-0.0854]), tensor([0.1157])), (tensor([0.6991]), tensor([0.1157])), (tensor([-0.3319]), tensor([0.1157])), (tensor([-0.6136]), tensor([0.1157]))], [(tensor([0.1720]), tensor([0.0570])), (tensor([-0.1330]), tensor([0.0570])), (tensor([0.3913]), tensor([0.0570])), (tensor([-1.5812]), tensor([0.0570])), (tensor([1.1261]), tensor([0.0570])), (tensor([-1.6258]), tensor([0.0570])), (tensor([0.1001]), tensor([0.0570])), (tensor([0.5129]), tensor([0.0570])), (tensor([-0.1273]), tensor([0.0570])), (tensor([0.1692]), tensor([0.0570]))], [(tensor([-0.9376]), tensor([0.0926])), (tensor([-2.0338]), tensor([0.0926])), (tensor([-0.2385]), tensor([0.0926])), (tensor([-0.2763]), tensor([0.0926])), (tensor([-1.1867]), tensor([0.0926])), (tensor([0.2528]), tensor([0.0926])), (tensor([-1.0881]), tensor([0.0926])), (tensor([0.5265]), tensor([0.0926])), (tensor([-0.9924]), tensor([0.0926])), (tensor([-1.4263]), tensor([0.0926]))], [(tensor([-0.7459]), tensor([0.0920])), (tensor([-0.2794]), tensor([0.0920])), (tensor([-0.1324]), tensor([0.0920])), (tensor([-2.4896]), tensor([0.0920])), (tensor([-0.4367]), tensor([0.0920])), (tensor([0.7143]), tensor([0.0920])), (tensor([-0.3027]), tensor([0.0920])), (tensor([1.0835]), tensor([0.0920])), (tensor([0.8076]), tensor([0.0920])), (tensor([0.8309]), tensor([0.0920]))], [(tensor([0.0097]), tensor([0.0906])), (tensor([0.0645]), tensor([0.0906])), (tensor([-1.0869]), tensor([0.0906])), (tensor([-0.9740]), tensor([0.0906])), (tensor([0.8430]), tensor([0.0906])), (tensor([0.9783]), tensor([0.0906])), (tensor([-1.3820]), tensor([0.0906])), (tensor([-0.4266]), tensor([0.0906])), (tensor([0.0159]), tensor([0.0906])), (tensor([-0.4855]), tensor([0.0906]))], [(tensor([1.2630]), tensor([0.0876])), (tensor([1.1857]), tensor([0.0876])), (tensor([-0.3966]), tensor([0.0876])), (tensor([0.0022]), tensor([0.0876])), (tensor([-0.5037]), tensor([0.0876])), (tensor([1.3446]), tensor([0.0876])), (tensor([0.4991]), tensor([0.0876])), (tensor([-1.1615]), tensor([0.0876])), (tensor([0.4874]), tensor([0.0876])), (tensor([-2.2340]), tensor([0.0876]))], [(tensor([1.2215]), tensor([0.0495])), (tensor([-0.4494]), tensor([0.0495])), (tensor([1.8269]), tensor([0.0495])), (tensor([0.2922]), tensor([0.0495])), (tensor([-1.1639]), tensor([0.0495])), (tensor([-0.7507]), tensor([0.0495])), (tensor([-1.2557]), tensor([0.0495])), (tensor([-2.1956]), tensor([0.0495])), (tensor([0.0913]), tensor([0.0495])), (tensor([1.2001]), tensor([0.0495]))], [(tensor([0.4814]), tensor([0.0588])), (tensor([0.0911]), tensor([0.0588])), (tensor([-1.5353]), tensor([0.0588])), (tensor([0.4369]), tensor([0.0588])), (tensor([0.1246]), tensor([0.0588])), (tensor([0.6022]), tensor([0.0588])), (tensor([-2.5375]), tensor([0.0588])), (tensor([1.3668]), tensor([0.0588])), (tensor([0.1561]), tensor([0.0588])), (tensor([1.2074]), tensor([0.0588]))], [(tensor([1.1296]), tensor([0.0553])), (tensor([0.8246]), tensor([0.0553])), (tensor([0.4529]), tensor([0.0553])), (tensor([-0.9485]), tensor([0.0553])), (tensor([0.7104]), tensor([0.0553])), (tensor([-0.1970]), tensor([0.0553])), (tensor([0.3377]), tensor([0.0553])), (tensor([0.8334]), tensor([0.0553])), (tensor([-0.7557]), tensor([0.0553])), (tensor([0.9233]), tensor([0.0553]))], [(tensor([0.4096]), tensor([0.0614])), (tensor([0.4609]), tensor([0.0614])), (tensor([1.3230]), tensor([0.0614])), (tensor([-0.0836]), tensor([0.0614])), (tensor([0.5907]), tensor([0.0614])), (tensor([-0.1822]), tensor([0.0614])), (tensor([-1.3570]), tensor([0.0614])), (tensor([-1.4997]), tensor([0.0614])), (tensor([-0.5275]), tensor([0.0614])), (tensor([-0.2082]), tensor([0.0614]))], [(tensor([0.9333]), tensor([0.0592])), (tensor([0.3675]), tensor([0.0592])), (tensor([2.1475]), tensor([0.0592])), (tensor([-0.3603]), tensor([0.0592])), (tensor([0.3695]), tensor([0.0592])), (tensor([0.2431]), tensor([0.0592])), (tensor([0.0391]), tensor([0.0592])), (tensor([0.3395]), tensor([0.0592])), (tensor([-0.8341]), tensor([0.0592])), (tensor([0.1354]), tensor([0.0592]))], [(tensor([0.0295]), tensor([0.1125])), (tensor([0.5042]), tensor([0.1125])), (tensor([0.4581]), tensor([0.1125])), (tensor([-1.3427]), tensor([0.1125])), (tensor([0.3335]), tensor([0.1125])), (tensor([-0.0836]), tensor([0.1125])), (tensor([0.5873]), tensor([0.1125])), (tensor([-0.4070]), tensor([0.1125])), (tensor([0.6164]), tensor([0.1125])), (tensor([0.6813]), tensor([0.1125]))], [(tensor([0.8234]), tensor([0.1157])), (tensor([1.4868]), tensor([0.1157])), (tensor([0.1148]), tensor([0.1157])), (tensor([1.0119]), tensor([0.1157])), (tensor([-0.2940]), tensor([0.1157])), (tensor([0.0703]), tensor([0.1157])), (tensor([1.8170]), tensor([0.1157])), (tensor([1.7630]), tensor([0.1157])), (tensor([-1.5991]), tensor([0.1157])), (tensor([0.8279]), tensor([0.1157]))], [(tensor([1.5587]), tensor([0.0875])), (tensor([-1.6384]), tensor([0.0875])), (tensor([-1.0532]), tensor([0.0875])), (tensor([-0.4944]), tensor([0.0875])), (tensor([1.5571]), tensor([0.0875])), (tensor([0.7944]), tensor([0.0875])), (tensor([-0.6489]), tensor([0.0875])), (tensor([-0.5657]), tensor([0.0875])), (tensor([-0.1659]), tensor([0.0875])), (tensor([-0.5926]), tensor([0.0875]))], [(tensor([-2.1422]), tensor([0.0974])), (tensor([-2.2538]), tensor([0.0974])), (tensor([-0.9142]), tensor([0.0974])), (tensor([0.7216]), tensor([0.0974])), (tensor([0.9589]), tensor([0.0974])), (tensor([-0.3011]), tensor([0.0974])), (tensor([-0.2952]), tensor([0.0974])), (tensor([0.5184]), tensor([0.0974])), (tensor([0.3155]), tensor([0.0974])), (tensor([2.1052]), tensor([0.0974]))], [(tensor([-0.5376]), tensor([0.1119])), (tensor([-0.4247]), tensor([0.1119])), (tensor([0.1541]), tensor([0.1119])), (tensor([-0.0156]), tensor([0.1119])), (tensor([-0.2740]), tensor([0.1119])), (tensor([0.3872]), tensor([0.1119])), (tensor([1.3892]), tensor([0.1119])), (tensor([0.3782]), tensor([0.1119])), (tensor([-2.1715]), tensor([0.1119])), (tensor([-1.8813]), tensor([0.1119]))], [(tensor([-0.3341]), tensor([0.1108])), (tensor([-1.8074]), tensor([0.1108])), (tensor([0.0247]), tensor([0.1108])), (tensor([2.6705]), tensor([0.1108])), (tensor([-0.0294]), tensor([0.1108])), (tensor([0.5612]), tensor([0.1108])), (tensor([0.8197]), tensor([0.1108])), (tensor([-0.8027]), tensor([0.1108])), (tensor([1.8565]), tensor([0.1108])), (tensor([-0.1922]), tensor([0.1108]))], [(tensor([-0.0128]), tensor([0.1157])), (tensor([-1.5807]), tensor([0.1157])), (tensor([-0.9893]), tensor([0.1157])), (tensor([-0.2774]), tensor([0.1157])), (tensor([0.2799]), tensor([0.1157])), (tensor([0.2877]), tensor([0.1157])), (tensor([-0.5722]), tensor([0.1157])), (tensor([-1.2411]), tensor([0.1157])), (tensor([0.7922]), tensor([0.1157])), (tensor([0.1292]), tensor([0.1157]))], [(tensor([1.9259]), tensor([0.1157])), (tensor([1.4228]), tensor([0.1157])), (tensor([0.9987]), tensor([0.1157])), (tensor([-0.1231]), tensor([0.1157])), (tensor([-0.3406]), tensor([0.1157])), (tensor([-0.5381]), tensor([0.1157])), (tensor([-0.2648]), tensor([0.1157])), (tensor([-1.3180]), tensor([0.1157])), (tensor([0.0451]), tensor([0.1157])), (tensor([1.0015]), tensor([0.1157]))], [(tensor([0.2678]), tensor([0.0739])), (tensor([1.8771]), tensor([0.0739])), (tensor([-1.2630]), tensor([0.0739])), (tensor([0.6614]), tensor([0.0739])), (tensor([-0.4017]), tensor([0.0739])), (tensor([0.1461]), tensor([0.0739])), (tensor([-1.0764]), tensor([0.0739])), (tensor([-0.4756]), tensor([0.0739])), (tensor([-0.4791]), tensor([0.0739])), (tensor([-1.1943]), tensor([0.0739]))], [(tensor([-0.1628]), tensor([0.1157])), (tensor([-0.6646]), tensor([0.1157])), (tensor([-0.4688]), tensor([0.1157])), (tensor([-1.6661]), tensor([0.1157])), (tensor([-0.9620]), tensor([0.1157])), (tensor([0.1341]), tensor([0.1157])), (tensor([1.4579]), tensor([0.1157])), (tensor([-0.9079]), tensor([0.1157])), (tensor([-1.1990]), tensor([0.1157])), (tensor([0.5211]), tensor([0.1157]))], [(tensor([-0.2192]), tensor([0.1007])), (tensor([-1.4342]), tensor([0.1007])), (tensor([-0.1584]), tensor([0.1007])), (tensor([-0.6690]), tensor([0.1007])), (tensor([0.0954]), tensor([0.1007])), (tensor([0.1205]), tensor([0.1007])), (tensor([-1.0195]), tensor([0.1007])), (tensor([1.0641]), tensor([0.1007])), (tensor([0.6816]), tensor([0.1007])), (tensor([0.2816]), tensor([0.1007]))], [(tensor([-0.0485]), tensor([0.0826])), (tensor([-1.5491]), tensor([0.0826])), (tensor([-1.1278]), tensor([0.0826])), (tensor([0.9314]), tensor([0.0826])), (tensor([0.3168]), tensor([0.0826])), (tensor([-1.0089]), tensor([0.0826])), (tensor([0.6975]), tensor([0.0826])), (tensor([0.7339]), tensor([0.0826])), (tensor([-1.1481]), tensor([0.0826])), (tensor([-0.6165]), tensor([0.0826]))], [(tensor([0.6102]), tensor([0.1059])), (tensor([1.8403]), tensor([0.1059])), (tensor([-0.0762]), tensor([0.1059])), (tensor([0.9181]), tensor([0.1059])), (tensor([-1.6728]), tensor([0.1059])), (tensor([0.6104]), tensor([0.1059])), (tensor([0.7065]), tensor([0.1059])), (tensor([0.6989]), tensor([0.1059])), (tensor([-0.2215]), tensor([0.1059])), (tensor([-0.0128]), tensor([0.1059]))], [(tensor([1.0769]), tensor([0.0752])), (tensor([1.0275]), tensor([0.0752])), (tensor([-0.1882]), tensor([0.0752])), (tensor([1.4148]), tensor([0.0752])), (tensor([-1.5908]), tensor([0.0752])), (tensor([0.6039]), tensor([0.0752])), (tensor([-0.0071]), tensor([0.0752])), (tensor([-0.2070]), tensor([0.0752])), (tensor([0.0497]), tensor([0.0752])), (tensor([0.3216]), tensor([0.0752]))], [(tensor([0.1424]), tensor([0.0848])), (tensor([-0.2828]), tensor([0.0848])), (tensor([0.5956]), tensor([0.0848])), (tensor([0.2904]), tensor([0.0848])), (tensor([0.1978]), tensor([0.0848])), (tensor([-1.0029]), tensor([0.0848])), (tensor([0.4437]), tensor([0.0848])), (tensor([-1.4764]), tensor([0.0848])), (tensor([-2.1020]), tensor([0.0848])), (tensor([-0.9558]), tensor([0.0848]))], [(tensor([-1.7560]), tensor([0.0456])), (tensor([0.8569]), tensor([0.0456])), (tensor([-0.0957]), tensor([0.0456])), (tensor([-0.1548]), tensor([0.0456])), (tensor([2.3401]), tensor([0.0456])), (tensor([-0.4732]), tensor([0.0456])), (tensor([-0.2734]), tensor([0.0456])), (tensor([0.8837]), tensor([0.0456])), (tensor([1.0039]), tensor([0.0456])), (tensor([0.8553]), tensor([0.0456]))], [(tensor([-2.1207]), tensor([0.0990])), (tensor([-0.7961]), tensor([0.0990])), (tensor([-1.4725]), tensor([0.0990])), (tensor([-0.3017]), tensor([0.0990])), (tensor([-0.4865]), tensor([0.0990])), (tensor([0.0066]), tensor([0.0990])), (tensor([-1.5170]), tensor([0.0990])), (tensor([-1.3195]), tensor([0.0990])), (tensor([0.4086]), tensor([0.0990])), (tensor([-0.9094]), tensor([0.0990]))], [(tensor([-1.5605]), tensor([0.1233])), (tensor([1.5354]), tensor([0.1233])), (tensor([0.3591]), tensor([0.1233])), (tensor([-0.6381]), tensor([0.1233])), (tensor([1.4751]), tensor([0.1233])), (tensor([-1.0541]), tensor([0.1233])), (tensor([0.3201]), tensor([0.1233])), (tensor([-2.0769]), tensor([0.1233])), (tensor([-0.7122]), tensor([0.1233])), (tensor([0.4308]), tensor([0.1233]))], [(tensor([-0.4074]), tensor([0.0754])), (tensor([-0.8267]), tensor([0.0754])), (tensor([0.3634]), tensor([0.0754])), (tensor([1.3257]), tensor([0.0754])), (tensor([0.5714]), tensor([0.0754])), (tensor([0.4037]), tensor([0.0754])), (tensor([-0.3950]), tensor([0.0754])), (tensor([-0.2649]), tensor([0.0754])), (tensor([0.2417]), tensor([0.0754])), (tensor([1.2743]), tensor([0.0754]))], [(tensor([-1.3020]), tensor([0.1111])), (tensor([1.4193]), tensor([0.1111])), (tensor([1.0250]), tensor([0.1111])), (tensor([-0.9297]), tensor([0.1111])), (tensor([0.1323]), tensor([0.1111])), (tensor([0.5172]), tensor([0.1111])), (tensor([1.3767]), tensor([0.1111])), (tensor([-0.4130]), tensor([0.1111])), (tensor([-0.0837]), tensor([0.1111])), (tensor([1.3487]), tensor([0.1111]))], [(tensor([0.0711]), tensor([0.0904])), (tensor([0.7405]), tensor([0.0904])), (tensor([-0.4050]), tensor([0.0904])), (tensor([0.8470]), tensor([0.0904])), (tensor([-0.9521]), tensor([0.0904])), (tensor([1.5996]), tensor([0.0904])), (tensor([2.1920]), tensor([0.0904])), (tensor([-0.3657]), tensor([0.0904])), (tensor([-0.0065]), tensor([0.0904])), (tensor([-0.9689]), tensor([0.0904]))], [(tensor([-0.0285]), tensor([0.1137])), (tensor([0.3801]), tensor([0.1137])), (tensor([-0.0306]), tensor([0.1137])), (tensor([1.4351]), tensor([0.1137])), (tensor([-0.1839]), tensor([0.1137])), (tensor([-0.2114]), tensor([0.1137])), (tensor([-1.5146]), tensor([0.1137])), (tensor([1.5822]), tensor([0.1137])), (tensor([0.7287]), tensor([0.1137])), (tensor([1.5931]), tensor([0.1137]))], [(tensor([-0.1625]), tensor([0.0505])), (tensor([1.4798]), tensor([0.0505])), (tensor([-1.6825]), tensor([0.0505])), (tensor([0.7690]), tensor([0.0505])), (tensor([-0.2231]), tensor([0.0505])), (tensor([-1.9729]), tensor([0.0505])), (tensor([0.9675]), tensor([0.0505])), (tensor([1.5370]), tensor([0.0505])), (tensor([2.1830]), tensor([0.0505])), (tensor([0.5735]), tensor([0.0505]))], [(tensor([-0.2271]), tensor([0.0280])), (tensor([-0.5629]), tensor([0.0280])), (tensor([-2.4936]), tensor([0.0280])), (tensor([1.0128]), tensor([0.0280])), (tensor([-1.1344]), tensor([0.0280])), (tensor([-0.2314]), tensor([0.0280])), (tensor([3.2813]), tensor([0.0280])), (tensor([0.3181]), tensor([0.0280])), (tensor([-2.7974]), tensor([0.0280])), (tensor([-0.3506]), tensor([0.0280]))], [(tensor([-0.2205]), tensor([0.1157])), (tensor([-1.0871]), tensor([0.1157])), (tensor([0.2233]), tensor([0.1157])), (tensor([-1.2009]), tensor([0.1157])), (tensor([1.4685]), tensor([0.1157])), (tensor([-0.0233]), tensor([0.1157])), (tensor([0.1389]), tensor([0.1157])), (tensor([-2.4848]), tensor([0.1157])), (tensor([1.3416]), tensor([0.1157])), (tensor([-0.0202]), tensor([0.1157]))], [(tensor([-0.3522]), tensor([0.0642])), (tensor([0.0822]), tensor([0.0642])), (tensor([1.1696]), tensor([0.0642])), (tensor([0.2445]), tensor([0.0642])), (tensor([1.9150]), tensor([0.0642])), (tensor([-1.2912]), tensor([0.0642])), (tensor([0.0332]), tensor([0.0642])), (tensor([0.6362]), tensor([0.0642])), (tensor([-0.0322]), tensor([0.0642])), (tensor([1.1134]), tensor([0.0642]))], [(tensor([0.1367]), tensor([0.1019])), (tensor([0.6693]), tensor([0.1019])), (tensor([0.4067]), tensor([0.1019])), (tensor([-1.6524]), tensor([0.1019])), (tensor([0.4683]), tensor([0.1019])), (tensor([-1.5656]), tensor([0.1019])), (tensor([0.7956]), tensor([0.1019])), (tensor([0.4417]), tensor([0.1019])), (tensor([-0.9139]), tensor([0.1019])), (tensor([-0.1047]), tensor([0.1019]))], [(tensor([-0.4368]), tensor([0.0978])), (tensor([-0.5288]), tensor([0.0978])), (tensor([0.0562]), tensor([0.0978])), (tensor([0.5790]), tensor([0.0978])), (tensor([1.7191]), tensor([0.0978])), (tensor([0.1073]), tensor([0.0978])), (tensor([-0.5969]), tensor([0.0978])), (tensor([-0.5133]), tensor([0.0978])), (tensor([-0.4149]), tensor([0.0978])), (tensor([1.2858]), tensor([0.0978]))], [(tensor([0.1244]), tensor([0.0737])), (tensor([0.5937]), tensor([0.0737])), (tensor([0.4184]), tensor([0.0737])), (tensor([1.9436]), tensor([0.0737])), (tensor([0.1412]), tensor([0.0737])), (tensor([0.7852]), tensor([0.0737])), (tensor([1.2118]), tensor([0.0737])), (tensor([-1.1811]), tensor([0.0737])), (tensor([-0.6635]), tensor([0.0737])), (tensor([0.7222]), tensor([0.0737]))], [(tensor([1.0106]), tensor([0.1157])), (tensor([0.9816]), tensor([0.1157])), (tensor([-0.3176]), tensor([0.1157])), (tensor([1.5760]), tensor([0.1157])), (tensor([-0.0218]), tensor([0.1157])), (tensor([1.4176]), tensor([0.1157])), (tensor([-0.6569]), tensor([0.1157])), (tensor([0.4026]), tensor([0.1157])), (tensor([-0.1732]), tensor([0.1157])), (tensor([0.3702]), tensor([0.1157]))], [(tensor([0.6861]), tensor([0.0794])), (tensor([-0.3207]), tensor([0.0794])), (tensor([-0.4721]), tensor([0.0794])), (tensor([0.0995]), tensor([0.0794])), (tensor([1.3899]), tensor([0.0794])), (tensor([1.6639]), tensor([0.0794])), (tensor([1.1838]), tensor([0.0794])), (tensor([0.0827]), tensor([0.0794])), (tensor([-1.3184]), tensor([0.0794])), (tensor([0.3777]), tensor([0.0794]))], [(tensor([0.9285]), tensor([0.1039])), (tensor([-0.0876]), tensor([0.1039])), (tensor([0.4821]), tensor([0.1039])), (tensor([1.8560]), tensor([0.1039])), (tensor([-1.2688]), tensor([0.1039])), (tensor([1.0197]), tensor([0.1039])), (tensor([-1.1113]), tensor([0.1039])), (tensor([-1.1862]), tensor([0.1039])), (tensor([-0.8803]), tensor([0.1039])), (tensor([-1.4697]), tensor([0.1039]))], [(tensor([0.4860]), tensor([0.1072])), (tensor([0.5382]), tensor([0.1072])), (tensor([-1.0117]), tensor([0.1072])), (tensor([0.1094]), tensor([0.1072])), (tensor([-0.3508]), tensor([0.1072])), (tensor([0.7571]), tensor([0.1072])), (tensor([-1.1815]), tensor([0.1072])), (tensor([-0.4276]), tensor([0.1072])), (tensor([-0.7984]), tensor([0.1072])), (tensor([0.2429]), tensor([0.1072]))], [(tensor([-0.6271]), tensor([0.0863])), (tensor([-0.7294]), tensor([0.0863])), (tensor([1.3465]), tensor([0.0863])), (tensor([0.3318]), tensor([0.0863])), (tensor([-0.5150]), tensor([0.0863])), (tensor([-0.7719]), tensor([0.0863])), (tensor([0.1248]), tensor([0.0863])), (tensor([-1.5279]), tensor([0.0863])), (tensor([-0.4879]), tensor([0.0863])), (tensor([0.8848]), tensor([0.0863]))], [(tensor([1.0140]), tensor([0.0895])), (tensor([1.3484]), tensor([0.0895])), (tensor([-1.4184]), tensor([0.0895])), (tensor([-0.1591]), tensor([0.0895])), (tensor([0.2080]), tensor([0.0895])), (tensor([1.8299]), tensor([0.0895])), (tensor([-1.4455]), tensor([0.0895])), (tensor([0.0992]), tensor([0.0895])), (tensor([-0.4895]), tensor([0.0895])), (tensor([-0.3784]), tensor([0.0895]))], [(tensor([-0.3179]), tensor([0.1195])), (tensor([-1.1578]), tensor([0.1195])), (tensor([0.4724]), tensor([0.1195])), (tensor([0.6377]), tensor([0.1195])), (tensor([-0.3861]), tensor([0.1195])), (tensor([-0.1483]), tensor([0.1195])), (tensor([-0.9560]), tensor([0.1195])), (tensor([-0.7505]), tensor([0.1195])), (tensor([-0.0986]), tensor([0.1195])), (tensor([-1.6496]), tensor([0.1195]))], [(tensor([-0.7245]), tensor([0.0439])), (tensor([1.4444]), tensor([0.0439])), (tensor([0.8148]), tensor([0.0439])), (tensor([-0.6508]), tensor([0.0439])), (tensor([-0.6691]), tensor([0.0439])), (tensor([0.3862]), tensor([0.0439])), (tensor([-1.1587]), tensor([0.0439])), (tensor([0.9656]), tensor([0.0439])), (tensor([-0.2303]), tensor([0.0439])), (tensor([-0.9173]), tensor([0.0439]))], [(tensor([-1.2557]), tensor([0.1001])), (tensor([-0.8660]), tensor([0.1001])), (tensor([-0.8658]), tensor([0.1001])), (tensor([-1.1663]), tensor([0.1001])), (tensor([0.2672]), tensor([0.1001])), (tensor([1.4167]), tensor([0.1001])), (tensor([-0.0136]), tensor([0.1001])), (tensor([-0.0981]), tensor([0.1001])), (tensor([0.2017]), tensor([0.1001])), (tensor([1.5390]), tensor([0.1001]))], [(tensor([0.2773]), tensor([0.0477])), (tensor([-0.2269]), tensor([0.0477])), (tensor([-0.4631]), tensor([0.0477])), (tensor([-2.6965]), tensor([0.0477])), (tensor([-1.7746]), tensor([0.0477])), (tensor([-1.6451]), tensor([0.0477])), (tensor([-0.5050]), tensor([0.0477])), (tensor([1.0459]), tensor([0.0477])), (tensor([1.4052]), tensor([0.0477])), (tensor([0.6808]), tensor([0.0477]))], [(tensor([-0.2972]), tensor([0.0973])), (tensor([-1.1120]), tensor([0.0973])), (tensor([-0.4626]), tensor([0.0973])), (tensor([0.2991]), tensor([0.0973])), (tensor([0.3564]), tensor([0.0973])), (tensor([-0.3384]), tensor([0.0973])), (tensor([-0.7369]), tensor([0.0973])), (tensor([1.4016]), tensor([0.0973])), (tensor([0.7649]), tensor([0.0973])), (tensor([0.8091]), tensor([0.0973]))], [(tensor([-0.6611]), tensor([0.0845])), (tensor([-1.3940]), tensor([0.0845])), (tensor([0.4511]), tensor([0.0845])), (tensor([1.1444]), tensor([0.0845])), (tensor([0.6885]), tensor([0.0845])), (tensor([-0.6244]), tensor([0.0845])), (tensor([-0.3215]), tensor([0.0845])), (tensor([0.1922]), tensor([0.0845])), (tensor([1.1017]), tensor([0.0845])), (tensor([-1.1994]), tensor([0.0845]))], [(tensor([-0.3449]), tensor([0.0760])), (tensor([1.3036]), tensor([0.0760])), (tensor([-1.3129]), tensor([0.0760])), (tensor([-0.4884]), tensor([0.0760])), (tensor([0.5218]), tensor([0.0760])), (tensor([-0.2883]), tensor([0.0760])), (tensor([-1.5771]), tensor([0.0760])), (tensor([-0.2284]), tensor([0.0760])), (tensor([0.9863]), tensor([0.0760])), (tensor([-0.2038]), tensor([0.0760]))], [(tensor([0.0092]), tensor([0.0792])), (tensor([-0.7995]), tensor([0.0792])), (tensor([-0.6226]), tensor([0.0792])), (tensor([1.4007]), tensor([0.0792])), (tensor([-0.8585]), tensor([0.0792])), (tensor([0.2226]), tensor([0.0792])), (tensor([1.2415]), tensor([0.0792])), (tensor([-0.8144]), tensor([0.0792])), (tensor([-0.2492]), tensor([0.0792])), (tensor([1.4118]), tensor([0.0792]))], [(tensor([-1.5452]), tensor([0.0933])), (tensor([0.9662]), tensor([0.0933])), (tensor([-0.6919]), tensor([0.0933])), (tensor([0.3699]), tensor([0.0933])), (tensor([-1.3593]), tensor([0.0933])), (tensor([-1.7925]), tensor([0.0933])), (tensor([1.3765]), tensor([0.0933])), (tensor([-0.2262]), tensor([0.0933])), (tensor([0.8963]), tensor([0.0933])), (tensor([1.5602]), tensor([0.0933]))], [(tensor([1.0131]), tensor([0.0928])), (tensor([-0.2724]), tensor([0.0928])), (tensor([0.3742]), tensor([0.0928])), (tensor([1.0248]), tensor([0.0928])), (tensor([-0.7181]), tensor([0.0928])), (tensor([-0.9942]), tensor([0.0928])), (tensor([1.4057]), tensor([0.0928])), (tensor([-0.0135]), tensor([0.0928])), (tensor([0.9129]), tensor([0.0928])), (tensor([0.2908]), tensor([0.0928]))], [(tensor([1.2652]), tensor([0.0501])), (tensor([1.4157]), tensor([0.0501])), (tensor([0.5710]), tensor([0.0501])), (tensor([0.5089]), tensor([0.0501])), (tensor([0.4623]), tensor([0.0501])), (tensor([-0.4266]), tensor([0.0501])), (tensor([-1.0453]), tensor([0.0501])), (tensor([0.1884]), tensor([0.0501])), (tensor([-1.4570]), tensor([0.0501])), (tensor([-0.1251]), tensor([0.0501]))], [(tensor([0.0248]), tensor([0.0659])), (tensor([-0.0850]), tensor([0.0659])), (tensor([1.1448]), tensor([0.0659])), (tensor([-1.0257]), tensor([0.0659])), (tensor([0.3727]), tensor([0.0659])), (tensor([0.8236]), tensor([0.0659])), (tensor([0.5207]), tensor([0.0659])), (tensor([1.4085]), tensor([0.0659])), (tensor([1.0597]), tensor([0.0659])), (tensor([0.3743]), tensor([0.0659]))], [(tensor([-1.3631]), tensor([0.0934])), (tensor([-1.1448]), tensor([0.0934])), (tensor([-0.9169]), tensor([0.0934])), (tensor([2.4734]), tensor([0.0934])), (tensor([-1.9204]), tensor([0.0934])), (tensor([1.2949]), tensor([0.0934])), (tensor([-0.8482]), tensor([0.0934])), (tensor([-0.5514]), tensor([0.0934])), (tensor([0.3115]), tensor([0.0934])), (tensor([-1.6948]), tensor([0.0934]))], [(tensor([0.0228]), tensor([0.1157])), (tensor([0.2009]), tensor([0.1157])), (tensor([0.2058]), tensor([0.1157])), (tensor([-0.7364]), tensor([0.1157])), (tensor([-0.0902]), tensor([0.1157])), (tensor([-0.6261]), tensor([0.1157])), (tensor([0.6324]), tensor([0.1157])), (tensor([-0.6151]), tensor([0.1157])), (tensor([-0.3485]), tensor([0.1157])), (tensor([-1.2788]), tensor([0.1157]))], [(tensor([-0.9527]), tensor([0.0326])), (tensor([0.4179]), tensor([0.0326])), (tensor([-1.9883]), tensor([0.0326])), (tensor([0.6485]), tensor([0.0326])), (tensor([0.1804]), tensor([0.0326])), (tensor([0.8000]), tensor([0.0326])), (tensor([-0.7357]), tensor([0.0326])), (tensor([0.8425]), tensor([0.0326])), (tensor([0.0259]), tensor([0.0326])), (tensor([1.3714]), tensor([0.0326]))], [(tensor([1.3768]), tensor([0.0924])), (tensor([0.5483]), tensor([0.0924])), (tensor([-0.2455]), tensor([0.0924])), (tensor([-0.3509]), tensor([0.0924])), (tensor([0.7360]), tensor([0.0924])), (tensor([-0.3057]), tensor([0.0924])), (tensor([-0.1481]), tensor([0.0924])), (tensor([-1.8255]), tensor([0.0924])), (tensor([0.0518]), tensor([0.0924])), (tensor([0.5184]), tensor([0.0924]))], [(tensor([0.7008]), tensor([0.0535])), (tensor([-1.1821]), tensor([0.0535])), (tensor([0.1679]), tensor([0.0535])), (tensor([0.5856]), tensor([0.0535])), (tensor([0.0257]), tensor([0.0535])), (tensor([0.2341]), tensor([0.0535])), (tensor([1.4438]), tensor([0.0535])), (tensor([1.5468]), tensor([0.0535])), (tensor([0.2281]), tensor([0.0535])), (tensor([1.3270]), tensor([0.0535]))], [(tensor([0.8598]), tensor([0.0454])), (tensor([1.1844]), tensor([0.0454])), (tensor([-0.8560]), tensor([0.0454])), (tensor([-0.4183]), tensor([0.0454])), (tensor([1.2515]), tensor([0.0454])), (tensor([1.0884]), tensor([0.0454])), (tensor([-1.0847]), tensor([0.0454])), (tensor([-0.5427]), tensor([0.0454])), (tensor([1.2855]), tensor([0.0454])), (tensor([1.9680]), tensor([0.0454]))], [(tensor([-0.9190]), tensor([0.1157])), (tensor([-0.9038]), tensor([0.1157])), (tensor([1.1392]), tensor([0.1157])), (tensor([0.8890]), tensor([0.1157])), (tensor([-0.8416]), tensor([0.1157])), (tensor([1.0369]), tensor([0.1157])), (tensor([-1.0432]), tensor([0.1157])), (tensor([-0.4804]), tensor([0.1157])), (tensor([1.7063]), tensor([0.1157])), (tensor([0.2962]), tensor([0.1157]))], [(tensor([0.3829]), tensor([0.0872])), (tensor([1.1568]), tensor([0.0872])), (tensor([-1.3354]), tensor([0.0872])), (tensor([-0.3686]), tensor([0.0872])), (tensor([0.3931]), tensor([0.0872])), (tensor([-0.7979]), tensor([0.0872])), (tensor([1.7327]), tensor([0.0872])), (tensor([0.1637]), tensor([0.0872])), (tensor([0.4297]), tensor([0.0872])), (tensor([-1.0402]), tensor([0.0872]))], [(tensor([0.4139]), tensor([0.0481])), (tensor([0.9048]), tensor([0.0481])), (tensor([-1.3115]), tensor([0.0481])), (tensor([0.6550]), tensor([0.0481])), (tensor([-0.2571]), tensor([0.0481])), (tensor([-1.0379]), tensor([0.0481])), (tensor([-2.1484]), tensor([0.0481])), (tensor([0.5172]), tensor([0.0481])), (tensor([-1.1203]), tensor([0.0481])), (tensor([1.1821]), tensor([0.0481]))], [(tensor([1.6483]), tensor([0.1016])), (tensor([-0.7472]), tensor([0.1016])), (tensor([-0.3840]), tensor([0.1016])), (tensor([1.1351]), tensor([0.1016])), (tensor([1.2653]), tensor([0.1016])), (tensor([1.3321]), tensor([0.1016])), (tensor([0.5464]), tensor([0.1016])), (tensor([-0.9475]), tensor([0.1016])), (tensor([-1.0140]), tensor([0.1016])), (tensor([-0.6916]), tensor([0.1016]))], [(tensor([-0.7486]), tensor([0.0477])), (tensor([-0.4233]), tensor([0.0477])), (tensor([-1.8128]), tensor([0.0477])), (tensor([-1.6816]), tensor([0.0477])), (tensor([-0.6452]), tensor([0.0477])), (tensor([0.3326]), tensor([0.0477])), (tensor([1.1890]), tensor([0.0477])), (tensor([0.2008]), tensor([0.0477])), (tensor([0.3681]), tensor([0.0477])), (tensor([-1.4163]), tensor([0.0477]))], [(tensor([0.1025]), tensor([0.0864])), (tensor([-1.2935]), tensor([0.0864])), (tensor([0.2120]), tensor([0.0864])), (tensor([0.3998]), tensor([0.0864])), (tensor([-1.4529]), tensor([0.0864])), (tensor([-0.3329]), tensor([0.0864])), (tensor([1.8157]), tensor([0.0864])), (tensor([-1.2064]), tensor([0.0864])), (tensor([0.7754]), tensor([0.0864])), (tensor([-2.1466]), tensor([0.0864]))], [(tensor([1.7701]), tensor([0.0813])), (tensor([0.4619]), tensor([0.0813])), (tensor([0.1189]), tensor([0.0813])), (tensor([-0.6976]), tensor([0.0813])), (tensor([2.9156]), tensor([0.0813])), (tensor([-0.0063]), tensor([0.0813])), (tensor([0.0895]), tensor([0.0813])), (tensor([-3.1208]), tensor([0.0813])), (tensor([2.2196]), tensor([0.0813])), (tensor([-0.8928]), tensor([0.0813]))], [(tensor([0.7602]), tensor([0.1040])), (tensor([0.4658]), tensor([0.1040])), (tensor([-1.0100]), tensor([0.1040])), (tensor([-0.3599]), tensor([0.1040])), (tensor([0.0935]), tensor([0.1040])), (tensor([1.0812]), tensor([0.1040])), (tensor([0.2840]), tensor([0.1040])), (tensor([0.4859]), tensor([0.1040])), (tensor([0.6426]), tensor([0.1040])), (tensor([0.0979]), tensor([0.1040]))], [(tensor([0.5321]), tensor([0.0824])), (tensor([1.5840]), tensor([0.0824])), (tensor([-0.4262]), tensor([0.0824])), (tensor([-0.7260]), tensor([0.0824])), (tensor([-0.5735]), tensor([0.0824])), (tensor([-0.3585]), tensor([0.0824])), (tensor([0.6798]), tensor([0.0824])), (tensor([0.1598]), tensor([0.0824])), (tensor([1.3587]), tensor([0.0824])), (tensor([-2.1061]), tensor([0.0824]))], [(tensor([0.8163]), tensor([0.1050])), (tensor([-0.8401]), tensor([0.1050])), (tensor([0.6277]), tensor([0.1050])), (tensor([0.6977]), tensor([0.1050])), (tensor([0.5295]), tensor([0.1050])), (tensor([-1.9574]), tensor([0.1050])), (tensor([-0.7916]), tensor([0.1050])), (tensor([-0.9738]), tensor([0.1050])), (tensor([-0.8705]), tensor([0.1050])), (tensor([-0.3132]), tensor([0.1050]))], [(tensor([0.0849]), tensor([0.1252])), (tensor([-1.9537]), tensor([0.1252])), (tensor([-0.6024]), tensor([0.1252])), (tensor([-0.6192]), tensor([0.1252])), (tensor([0.2028]), tensor([0.1252])), (tensor([-0.7548]), tensor([0.1252])), (tensor([-2.0777]), tensor([0.1252])), (tensor([-0.1084]), tensor([0.1252])), (tensor([0.0283]), tensor([0.1252])), (tensor([-0.5437]), tensor([0.1252]))], [(tensor([0.7841]), tensor([0.0557])), (tensor([0.0686]), tensor([0.0557])), (tensor([0.5422]), tensor([0.0557])), (tensor([0.2030]), tensor([0.0557])), (tensor([-1.3422]), tensor([0.0557])), (tensor([0.5828]), tensor([0.0557])), (tensor([0.3281]), tensor([0.0557])), (tensor([1.4685]), tensor([0.0557])), (tensor([-0.1286]), tensor([0.0557])), (tensor([-1.6609]), tensor([0.0557]))], [(tensor([0.9887]), tensor([0.1275])), (tensor([-0.2872]), tensor([0.1275])), (tensor([1.6263]), tensor([0.1275])), (tensor([0.8180]), tensor([0.1275])), (tensor([0.9300]), tensor([0.1275])), (tensor([-0.1177]), tensor([0.1275])), (tensor([-0.7206]), tensor([0.1275])), (tensor([-0.1224]), tensor([0.1275])), (tensor([0.8805]), tensor([0.1275])), (tensor([-0.2954]), tensor([0.1275]))], [(tensor([-0.5957]), tensor([0.0983])), (tensor([0.7556]), tensor([0.0983])), (tensor([-1.6739]), tensor([0.0983])), (tensor([-0.7903]), tensor([0.0983])), (tensor([-2.6937]), tensor([0.0983])), (tensor([-0.3961]), tensor([0.0983])), (tensor([-1.2200]), tensor([0.0983])), (tensor([-0.0254]), tensor([0.0983])), (tensor([-0.3351]), tensor([0.0983])), (tensor([1.4519]), tensor([0.0983]))], [(tensor([-2.2720]), tensor([0.0915])), (tensor([-0.9333]), tensor([0.0915])), (tensor([0.6390]), tensor([0.0915])), (tensor([0.7999]), tensor([0.0915])), (tensor([0.9900]), tensor([0.0915])), (tensor([0.6977]), tensor([0.0915])), (tensor([-0.8817]), tensor([0.0915])), (tensor([0.7852]), tensor([0.0915])), (tensor([0.7802]), tensor([0.0915])), (tensor([0.4764]), tensor([0.0915]))], [(tensor([-0.1153]), tensor([0.0525])), (tensor([-0.7719]), tensor([0.0525])), (tensor([-1.2491]), tensor([0.0525])), (tensor([-0.3934]), tensor([0.0525])), (tensor([2.9401]), tensor([0.0525])), (tensor([0.4258]), tensor([0.0525])), (tensor([0.1776]), tensor([0.0525])), (tensor([0.3755]), tensor([0.0525])), (tensor([0.3931]), tensor([0.0525])), (tensor([0.5934]), tensor([0.0525]))], [(tensor([0.3333]), tensor([0.0995])), (tensor([-0.6777]), tensor([0.0995])), (tensor([-1.1028]), tensor([0.0995])), (tensor([-0.6987]), tensor([0.0995])), (tensor([-2.1713]), tensor([0.0995])), (tensor([0.7588]), tensor([0.0995])), (tensor([-1.5629]), tensor([0.0995])), (tensor([0.0484]), tensor([0.0995])), (tensor([0.2601]), tensor([0.0995])), (tensor([0.0119]), tensor([0.0995]))], [(tensor([-0.5166]), tensor([0.0908])), (tensor([-0.3992]), tensor([0.0908])), (tensor([-0.4814]), tensor([0.0908])), (tensor([-0.2643]), tensor([0.0908])), (tensor([-0.4581]), tensor([0.0908])), (tensor([-1.3763]), tensor([0.0908])), (tensor([-0.4990]), tensor([0.0908])), (tensor([0.1954]), tensor([0.0908])), (tensor([-0.4799]), tensor([0.0908])), (tensor([-0.3018]), tensor([0.0908]))], [(tensor([0.5717]), tensor([0.1223])), (tensor([-0.5939]), tensor([0.1223])), (tensor([-0.9081]), tensor([0.1223])), (tensor([-0.6230]), tensor([0.1223])), (tensor([-0.5412]), tensor([0.1223])), (tensor([0.1820]), tensor([0.1223])), (tensor([-1.6096]), tensor([0.1223])), (tensor([-0.1145]), tensor([0.1223])), (tensor([-0.5836]), tensor([0.1223])), (tensor([1.7015]), tensor([0.1223]))], [(tensor([-0.8155]), tensor([0.0884])), (tensor([-0.6057]), tensor([0.0884])), (tensor([-0.1901]), tensor([0.0884])), (tensor([1.1099]), tensor([0.0884])), (tensor([1.2277]), tensor([0.0884])), (tensor([-0.6361]), tensor([0.0884])), (tensor([-0.3691]), tensor([0.0884])), (tensor([-0.1099]), tensor([0.0884])), (tensor([0.6552]), tensor([0.0884])), (tensor([-0.0715]), tensor([0.0884]))], [(tensor([-2.7708]), tensor([0.0910])), (tensor([1.3596]), tensor([0.0910])), (tensor([1.1938]), tensor([0.0910])), (tensor([-0.7438]), tensor([0.0910])), (tensor([1.2383]), tensor([0.0910])), (tensor([0.1173]), tensor([0.0910])), (tensor([1.2155]), tensor([0.0910])), (tensor([-0.9260]), tensor([0.0910])), (tensor([-0.6305]), tensor([0.0910])), (tensor([0.1102]), tensor([0.0910]))], [(tensor([-1.2624]), tensor([0.1045])), (tensor([0.4269]), tensor([0.1045])), (tensor([-2.2169]), tensor([0.1045])), (tensor([-1.0168]), tensor([0.1045])), (tensor([-1.3375]), tensor([0.1045])), (tensor([-1.1839]), tensor([0.1045])), (tensor([-0.2288]), tensor([0.1045])), (tensor([0.6673]), tensor([0.1045])), (tensor([-1.0245]), tensor([0.1045])), (tensor([0.5220]), tensor([0.1045]))], [(tensor([0.2471]), tensor([0.0581])), (tensor([-0.4055]), tensor([0.0581])), (tensor([-0.9091]), tensor([0.0581])), (tensor([-0.6467]), tensor([0.0581])), (tensor([-0.2849]), tensor([0.0581])), (tensor([0.1394]), tensor([0.0581])), (tensor([-1.6959]), tensor([0.0581])), (tensor([0.1827]), tensor([0.0581])), (tensor([-1.7794]), tensor([0.0581])), (tensor([0.6686]), tensor([0.0581]))], [(tensor([-0.0762]), tensor([0.1109])), (tensor([-0.5747]), tensor([0.1109])), (tensor([-0.4215]), tensor([0.1109])), (tensor([1.8006]), tensor([0.1109])), (tensor([-0.0374]), tensor([0.1109])), (tensor([1.7642]), tensor([0.1109])), (tensor([0.7196]), tensor([0.1109])), (tensor([0.8383]), tensor([0.1109])), (tensor([0.0793]), tensor([0.1109])), (tensor([0.0140]), tensor([0.1109]))], [(tensor([0.4413]), tensor([0.1157])), (tensor([1.2739]), tensor([0.1157])), (tensor([-0.8662]), tensor([0.1157])), (tensor([-0.3641]), tensor([0.1157])), (tensor([-0.7297]), tensor([0.1157])), (tensor([-1.6808]), tensor([0.1157])), (tensor([-1.3104]), tensor([0.1157])), (tensor([-1.2666]), tensor([0.1157])), (tensor([-1.5399]), tensor([0.1157])), (tensor([-0.3429]), tensor([0.1157]))], [(tensor([1.4354]), tensor([0.0609])), (tensor([-1.6708]), tensor([0.0609])), (tensor([-0.9836]), tensor([0.0609])), (tensor([1.2450]), tensor([0.0609])), (tensor([-0.3683]), tensor([0.0609])), (tensor([-0.3175]), tensor([0.0609])), (tensor([0.3605]), tensor([0.0609])), (tensor([1.9843]), tensor([0.0609])), (tensor([-0.4464]), tensor([0.0609])), (tensor([-1.3883]), tensor([0.0609]))], [(tensor([-1.2451]), tensor([0.1026])), (tensor([-0.3015]), tensor([0.1026])), (tensor([1.1474]), tensor([0.1026])), (tensor([-0.6574]), tensor([0.1026])), (tensor([-0.5490]), tensor([0.1026])), (tensor([-0.3163]), tensor([0.1026])), (tensor([0.4368]), tensor([0.1026])), (tensor([-0.4262]), tensor([0.1026])), (tensor([0.2168]), tensor([0.1026])), (tensor([0.6076]), tensor([0.1026]))], [(tensor([-0.7601]), tensor([0.0856])), (tensor([-0.3631]), tensor([0.0856])), (tensor([0.0984]), tensor([0.0856])), (tensor([2.2234]), tensor([0.0856])), (tensor([-0.0165]), tensor([0.0856])), (tensor([-0.5872]), tensor([0.0856])), (tensor([0.9508]), tensor([0.0856])), (tensor([-0.3827]), tensor([0.0856])), (tensor([-0.4519]), tensor([0.0856])), (tensor([-0.6886]), tensor([0.0856]))], [(tensor([0.2294]), tensor([0.1098])), (tensor([1.4112]), tensor([0.1098])), (tensor([-1.6916]), tensor([0.1098])), (tensor([-0.8997]), tensor([0.1098])), (tensor([-0.4657]), tensor([0.1098])), (tensor([-0.8099]), tensor([0.1098])), (tensor([-1.8636]), tensor([0.1098])), (tensor([0.6629]), tensor([0.1098])), (tensor([0.0744]), tensor([0.1098])), (tensor([0.1686]), tensor([0.1098]))], [(tensor([-0.1624]), tensor([0.1100])), (tensor([-1.6879]), tensor([0.1100])), (tensor([1.9725]), tensor([0.1100])), (tensor([-0.3255]), tensor([0.1100])), (tensor([-0.5773]), tensor([0.1100])), (tensor([0.5081]), tensor([0.1100])), (tensor([-1.5774]), tensor([0.1100])), (tensor([0.0428]), tensor([0.1100])), (tensor([0.0410]), tensor([0.1100])), (tensor([-0.7495]), tensor([0.1100]))], [(tensor([0.4452]), tensor([0.0805])), (tensor([-1.2517]), tensor([0.0805])), (tensor([-0.1934]), tensor([0.0805])), (tensor([0.3726]), tensor([0.0805])), (tensor([-0.7575]), tensor([0.0805])), (tensor([0.8606]), tensor([0.0805])), (tensor([-1.4408]), tensor([0.0805])), (tensor([-0.7137]), tensor([0.0805])), (tensor([0.8899]), tensor([0.0805])), (tensor([0.1649]), tensor([0.0805]))]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now each whole_mask has its corresponding context. That is\n",
        "For mask_i , context_i is defined [(x_1, masked_nn(x_1)), ... (x_n, masked_nn(x_n))]. So each mask has n context points.\n",
        "\n",
        "Now we make an autoencoder to encode/decode whole masks."
      ],
      "metadata": {
        "id": "SuxVmghRGkTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 32\n",
        "\n",
        "def prepare_masks_for_embed(whole_mask_list):\n",
        "      \"\"\"whole_mask_list is list of whole masks, whole mask is list of per layer masks\"\"\"\n",
        "      flattened_masks = []\n",
        "      for whole_mask in whole_mask_list:\n",
        "          flat = torch.cat([mask.flatten() for mask in whole_mask], dim=0)  # shape [D] where D = total number of weights in network\n",
        "          flattened_masks.append(flat)\n",
        "\n",
        "      return torch.stack(flattened_masks)  # shape [num_masks, D]\n",
        "\n",
        "class WholeMaskAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode masks for entire network ie whole masks,\n",
        "    inputs generated by prepare_mask_for_embed(whole_mask_list)\n",
        "    \"\"\"\n",
        "    def __init__(self, D, embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(D, max(128, embed_dim*2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(max(128, embed_dim*2), embed_dim),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, max(128, embed_dim*2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(max(128, embed_dim*2), D)   # raw logits for masks\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "        return y\n",
        "\n",
        "    def encode(self, x):  # x: [B, D]\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):  # z: [B, E]\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "eCeHrNociN9d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the autoencoder"
      ],
      "metadata": {
        "id": "q2KBjtiDjQmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_mask_tensor = prepare_masks_for_embed(whole_masks_list)  # shape [num_masks, D] where D is total number of weights in network\n",
        "\n",
        "D = prepared_mask_tensor.shape[1]\n",
        "\n",
        "mask_ae = WholeMaskAE(D, EMBED_DIM)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(mask_ae.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    recon = mask_ae(prepared_mask_tensor)  # AE tries to reconstruct input\n",
        "    loss = criterion(recon, prepared_mask_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "i1LvOeb7jPW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e42cd5a-cf9d-4b88-d49c-2b0c7aee4802"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss = 0.3609\n",
            "Epoch 20: Loss = 0.2541\n",
            "Epoch 30: Loss = 0.2446\n",
            "Epoch 40: Loss = 0.2414\n",
            "Epoch 50: Loss = 0.2398\n",
            "Epoch 60: Loss = 0.2377\n",
            "Epoch 70: Loss = 0.2339\n",
            "Epoch 80: Loss = 0.2273\n",
            "Epoch 90: Loss = 0.2188\n",
            "Epoch 100: Loss = 0.2101\n",
            "Epoch 110: Loss = 0.2022\n",
            "Epoch 120: Loss = 0.1933\n",
            "Epoch 130: Loss = 0.1858\n",
            "Epoch 140: Loss = 0.1785\n",
            "Epoch 150: Loss = 0.1724\n",
            "Epoch 160: Loss = 0.1680\n",
            "Epoch 170: Loss = 0.1647\n",
            "Epoch 180: Loss = 0.1616\n",
            "Epoch 190: Loss = 0.1583\n",
            "Epoch 200: Loss = 0.1556\n",
            "Epoch 210: Loss = 0.1525\n",
            "Epoch 220: Loss = 0.1501\n",
            "Epoch 230: Loss = 0.1478\n",
            "Epoch 240: Loss = 0.1457\n",
            "Epoch 250: Loss = 0.1437\n",
            "Epoch 260: Loss = 0.1418\n",
            "Epoch 270: Loss = 0.1394\n",
            "Epoch 280: Loss = 0.1372\n",
            "Epoch 290: Loss = 0.1350\n",
            "Epoch 300: Loss = 0.1327\n",
            "Epoch 310: Loss = 0.1303\n",
            "Epoch 320: Loss = 0.1279\n",
            "Epoch 330: Loss = 0.1255\n",
            "Epoch 340: Loss = 0.1230\n",
            "Epoch 350: Loss = 0.1207\n",
            "Epoch 360: Loss = 0.1182\n",
            "Epoch 370: Loss = 0.1157\n",
            "Epoch 380: Loss = 0.1131\n",
            "Epoch 390: Loss = 0.1107\n",
            "Epoch 400: Loss = 0.1078\n",
            "Epoch 410: Loss = 0.1050\n",
            "Epoch 420: Loss = 0.1020\n",
            "Epoch 430: Loss = 0.0992\n",
            "Epoch 440: Loss = 0.0965\n",
            "Epoch 450: Loss = 0.0940\n",
            "Epoch 460: Loss = 0.0912\n",
            "Epoch 470: Loss = 0.0887\n",
            "Epoch 480: Loss = 0.0862\n",
            "Epoch 490: Loss = 0.0835\n",
            "Epoch 500: Loss = 0.0812\n",
            "Epoch 510: Loss = 0.0787\n",
            "Epoch 520: Loss = 0.0763\n",
            "Epoch 530: Loss = 0.0739\n",
            "Epoch 540: Loss = 0.0718\n",
            "Epoch 550: Loss = 0.0692\n",
            "Epoch 560: Loss = 0.0674\n",
            "Epoch 570: Loss = 0.0650\n",
            "Epoch 580: Loss = 0.0628\n",
            "Epoch 590: Loss = 0.0610\n",
            "Epoch 600: Loss = 0.0586\n",
            "Epoch 610: Loss = 0.0567\n",
            "Epoch 620: Loss = 0.0547\n",
            "Epoch 630: Loss = 0.0527\n",
            "Epoch 640: Loss = 0.0512\n",
            "Epoch 650: Loss = 0.0490\n",
            "Epoch 660: Loss = 0.0472\n",
            "Epoch 670: Loss = 0.0464\n",
            "Epoch 680: Loss = 0.0454\n",
            "Epoch 690: Loss = 0.0426\n",
            "Epoch 700: Loss = 0.0410\n",
            "Epoch 710: Loss = 0.0402\n",
            "Epoch 720: Loss = 0.0386\n",
            "Epoch 730: Loss = 0.0371\n",
            "Epoch 740: Loss = 0.0361\n",
            "Epoch 750: Loss = 0.0347\n",
            "Epoch 760: Loss = 0.0342\n",
            "Epoch 770: Loss = 0.0325\n",
            "Epoch 780: Loss = 0.0313\n",
            "Epoch 790: Loss = 0.0309\n",
            "Epoch 800: Loss = 0.0298\n",
            "Epoch 810: Loss = 0.0284\n",
            "Epoch 820: Loss = 0.0276\n",
            "Epoch 830: Loss = 0.0272\n",
            "Epoch 840: Loss = 0.0259\n",
            "Epoch 850: Loss = 0.0248\n",
            "Epoch 860: Loss = 0.0250\n",
            "Epoch 870: Loss = 0.0234\n",
            "Epoch 880: Loss = 0.0224\n",
            "Epoch 890: Loss = 0.0220\n",
            "Epoch 900: Loss = 0.0214\n",
            "Epoch 910: Loss = 0.0206\n",
            "Epoch 920: Loss = 0.0198\n",
            "Epoch 930: Loss = 0.0202\n",
            "Epoch 940: Loss = 0.0192\n",
            "Epoch 950: Loss = 0.0180\n",
            "Epoch 960: Loss = 0.0177\n",
            "Epoch 970: Loss = 0.0181\n",
            "Epoch 980: Loss = 0.0170\n",
            "Epoch 990: Loss = 0.0160\n",
            "Epoch 1000: Loss = 0.0155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_data = []\n",
        "for i in range(len(prepared_mask_tensor)):\n",
        "  context = whole_masks_context_list[i]\n",
        "\n",
        "  flattened_context = torch.cat([torch.cat((j[0], j[1])) for j in context])\n",
        "\n",
        "  encoded_mask = mask_ae.encode(prepared_mask_tensor[i].unsqueeze(0)).squeeze(0).detach() #break comp graph from ae\n",
        "\n",
        "  training_data.append((flattened_context, encoded_mask))\n",
        "\n",
        "print(training_data)\n",
        "\n",
        "# training data is pairs of (flattened_context, autoencoder(respective mask))\n",
        "\n",
        "# so we are basically saying \"here's what this function should look like\"\n",
        "# and \"give me the vector that will get decoded to be the mask to approximate the function\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQun6uHCGcVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ae5b7f-e135-4bf2-e30f-da63698cbfcd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor([-0.1589,  0.0911, -1.8219,  0.0911,  0.9176,  0.0911, -0.3174,  0.0911,\n",
            "         0.0772,  0.0911,  1.5879,  0.0911,  0.0347,  0.0911,  0.6774,  0.0911,\n",
            "        -0.3319,  0.0911,  2.0134,  0.0911]), tensor([  5.0334,   5.3987,  -9.4939,  -5.3220,   3.1807,   2.0935,   3.0375,\n",
            "         -2.1661,  -7.1258,  -2.2957, -13.2378,  13.0512,  -8.1087,   2.2995,\n",
            "          5.2444,   6.7891,   1.0956,   4.8833,   8.1026, -10.7583,  11.7555,\n",
            "          2.1992,   2.5882,   1.4380,   4.0920,   5.6199,  13.8963,   8.4806,\n",
            "         -2.1295,  14.0702,   1.6270,  12.5810])), (tensor([ 0.1815,  0.1235, -1.4175,  0.1235,  0.4602,  0.1235, -2.0951,  0.1235,\n",
            "         1.1751,  0.1235,  0.7389,  0.1235,  0.1005,  0.1235,  0.9139,  0.1235,\n",
            "         0.1667,  0.1235,  0.6655,  0.1235]), tensor([ 3.7202,  9.0669, -6.4105, -2.4829, -0.5054, -0.6486,  6.4054,  3.1014,\n",
            "        -7.4661, -1.9161, -3.4143,  2.5878, -6.7323, -0.5869,  4.3324,  0.9137,\n",
            "         0.3167,  1.7482,  6.8083, -3.2109, 11.8243,  5.2103, -1.0808, -2.5040,\n",
            "        -1.9669,  0.1457, 10.8539,  5.5716, -0.8827,  6.4682,  1.9652,  8.1694])), (tensor([ 0.5444,  0.0634, -0.3648,  0.0634,  0.1937,  0.0634,  0.1725,  0.0634,\n",
            "         0.3115,  0.0634, -0.2265,  0.0634, -1.5003,  0.0634,  0.5010,  0.0634,\n",
            "         0.8823,  0.0634,  1.5086,  0.0634]), tensor([  5.0317,  -8.5922,  -7.3219,   5.0927,   0.5234,  -3.1185,   8.2193,\n",
            "         -7.8571,  -9.8515,  -3.5965,  -2.0348,   9.4931,  -5.9147,  -6.0215,\n",
            "          5.4719,   4.6559, -12.2784,   4.7201,   6.3425, -10.4726,  -5.9919,\n",
            "         -1.0769,   2.1795,   0.7791,  -1.2470,   1.8705,  15.8422,  10.1442,\n",
            "         -0.6036,  -2.8459,   3.6407,   3.8047])), (tensor([-1.3234,  0.0685, -0.7790,  0.0685,  2.6751,  0.0685,  0.0961,  0.0685,\n",
            "         0.7835,  0.0685,  0.6140,  0.0685,  0.7490,  0.0685,  0.3745,  0.0685,\n",
            "        -1.4189,  0.0685,  0.8427,  0.0685]), tensor([ 0.6694,  2.8538, -5.9007,  5.2734,  9.4534,  1.1287,  8.2514, -5.0734,\n",
            "        -3.0976, -5.8726, -7.7736, 14.0821, -9.9679,  0.7188,  3.0504,  9.0984,\n",
            "        -9.0093, 13.2708, 10.4599, -9.9453,  4.5676,  1.1947, -0.6258,  6.4577,\n",
            "         5.4966,  7.3780, 14.2414,  5.6296, -4.9337,  7.1012,  0.7462, 14.5695])), (tensor([ 0.6131,  0.0732, -0.0172,  0.0732,  0.3057,  0.0732,  0.8939,  0.0732,\n",
            "        -0.3765,  0.0732,  0.0570,  0.0732,  0.1223,  0.0732, -0.9210,  0.0732,\n",
            "        -0.8104,  0.0732, -0.8906,  0.0732]), tensor([ -2.3494,  -1.2110, -18.1672,  -4.7228,  -0.6640,  -2.1529,   8.2921,\n",
            "         -0.4031,  -6.3625,  -7.8000,  -4.5003,  13.5096,  -9.2728, -11.9763,\n",
            "          6.5226,   7.8751,  -9.9816,   9.1459,  -4.0795,  -3.7627,   4.1348,\n",
            "         -5.3252,  -2.7527,   2.3941,   3.1560,  -8.2194,  18.9623,   8.6659,\n",
            "         -3.1790,   1.5885,  -4.2281,  18.4293])), (tensor([ 0.4580,  0.1157,  0.4919,  0.1157, -1.6342,  0.1157, -0.2046,  0.1157,\n",
            "         0.1812,  0.1157,  0.5255,  0.1157, -0.0854,  0.1157,  0.6991,  0.1157,\n",
            "        -0.3319,  0.1157, -0.6136,  0.1157]), tensor([ -3.0385,   3.5362,  -4.1213,   6.2551,   5.1574,   1.1358,  11.5530,\n",
            "          0.4118,  -4.5092,  -5.1815,   2.1775,   5.5417,  -8.8139,  -9.3467,\n",
            "          5.3418,   3.8349, -11.8276,   9.1602,   6.4977,  -2.5379,   4.5152,\n",
            "         -0.5100,  -5.6494,   4.2429,   0.7723,  -3.4488,  12.8991,   2.2497,\n",
            "         -3.3662,  -3.0251,   0.3389,  11.4895])), (tensor([ 0.1720,  0.0570, -0.1330,  0.0570,  0.3913,  0.0570, -1.5812,  0.0570,\n",
            "         1.1261,  0.0570, -1.6258,  0.0570,  0.1001,  0.0570,  0.5129,  0.0570,\n",
            "        -0.1273,  0.0570,  0.1692,  0.0570]), tensor([  2.1179,  -0.0816,  -8.8136,  -4.2048,   2.4737,  -0.1855,   1.2047,\n",
            "         -5.7998,  -0.8842,  -5.0384, -11.2896,  17.1273,  -4.0854,   1.5327,\n",
            "          0.5142,  12.2651,  -3.5521,  11.8999,  -0.5474,  -7.7962,   2.2011,\n",
            "          0.3535,   1.1412,   8.3123,   7.3666,   5.2238,   9.5730,   7.4985,\n",
            "         -2.3647,   9.9853,  -0.9891,  14.5531])), (tensor([-0.9376,  0.0926, -2.0338,  0.0926, -0.2385,  0.0926, -0.2763,  0.0926,\n",
            "        -1.1867,  0.0926,  0.2528,  0.0926, -1.0881,  0.0926,  0.5265,  0.0926,\n",
            "        -0.9924,  0.0926, -1.4263,  0.0926]), tensor([  3.7310,  -8.5007, -11.3427,   0.9824,  -1.7937,  -0.5628,   6.5401,\n",
            "         -3.3798, -10.7290,  -2.3653,  -2.0258,   8.3215,  -6.1341, -12.8649,\n",
            "          9.6619,   1.4490, -11.5678,  -0.1941,   1.4003,  -7.4539,  -2.5367,\n",
            "         -3.8981,   0.4569,  -1.6380,  -2.1687,  -3.7242,  16.6556,  10.1168,\n",
            "         -0.7622,  -2.2510,  -0.4000,   6.3612])), (tensor([-0.7459,  0.0920, -0.2794,  0.0920, -0.1324,  0.0920, -2.4896,  0.0920,\n",
            "        -0.4367,  0.0920,  0.7143,  0.0920, -0.3027,  0.0920,  1.0835,  0.0920,\n",
            "         0.8076,  0.0920,  0.8309,  0.0920]), tensor([  0.7489,  -3.7585, -17.4412,  -1.7041,   5.0213,  -8.1935,   3.3872,\n",
            "         -5.1782,  -7.2428,  -7.7646,  -8.2857,   7.4813,  -6.4764,  -0.6284,\n",
            "          2.7663,   3.3135,  -5.8970,   3.7350,   2.0216,  -7.4473,  -3.9825,\n",
            "         -0.7277,   7.2340,  -3.0574,   0.9149,   2.4950,  12.1612,   7.2595,\n",
            "         -3.8547,   4.3516,  -9.1377,   7.4286])), (tensor([ 0.0097,  0.0906,  0.0645,  0.0906, -1.0869,  0.0906, -0.9740,  0.0906,\n",
            "         0.8430,  0.0906,  0.9783,  0.0906, -1.3820,  0.0906, -0.4266,  0.0906,\n",
            "         0.0159,  0.0906, -0.4855,  0.0906]), tensor([  0.3203,  -4.1129,  -8.0957,  -1.8547,   8.9842,   3.2001,  -0.7280,\n",
            "         -7.9363,  -2.4384,  -3.8057, -14.3954,  16.9829,  -5.2089,  -1.0426,\n",
            "          4.2603,   9.6965,  -4.7748,   7.7722,   4.1963, -12.0403,   0.0926,\n",
            "         -3.6624,   4.3926,   7.7782,   8.8488,   6.8890,   9.1044,   4.9912,\n",
            "         -3.7917,  10.0602,  -4.1831,  11.2240])), (tensor([ 1.2630e+00,  8.7574e-02,  1.1857e+00,  8.7574e-02, -3.9659e-01,\n",
            "         8.7574e-02,  2.1998e-03,  8.7574e-02, -5.0367e-01,  8.7574e-02,\n",
            "         1.3446e+00,  8.7574e-02,  4.9909e-01,  8.7574e-02, -1.1615e+00,\n",
            "         8.7574e-02,  4.8743e-01,  8.7574e-02, -2.2340e+00,  8.7574e-02]), tensor([  7.0786,  -5.5702, -13.3287,  -6.8427,  -2.8921,  -5.2767,  -0.0822,\n",
            "         -7.9720,  -7.5254,  -4.5741, -11.2508,  14.3900,  -1.2618,  -0.7415,\n",
            "          2.4336,   9.3619,  -3.9194,   5.3964,  -2.7196,  -9.2554,  -3.7893,\n",
            "          2.2438,   5.8513,   3.5346,   2.2922,   4.1748,  10.8938,  12.3371,\n",
            "          0.1100,   7.7239,  -2.4575,   8.5200])), (tensor([ 1.2215,  0.0495, -0.4494,  0.0495,  1.8269,  0.0495,  0.2922,  0.0495,\n",
            "        -1.1639,  0.0495, -0.7507,  0.0495, -1.2557,  0.0495, -2.1956,  0.0495,\n",
            "         0.0913,  0.0495,  1.2001,  0.0495]), tensor([ -0.3912,  -1.7716, -16.1535,  -7.7332,   0.3645, -12.4731,   5.8888,\n",
            "         -8.5850,  -9.9299,  -9.6283,  -8.0782,   8.6094,  -5.5572,   3.7574,\n",
            "         -2.3962,   9.9122,  -0.1587,   8.3531,   3.1844,  -9.7716,  -2.5745,\n",
            "         -1.5541,   9.0134,  -0.6274,   2.9736,  -2.7824,  13.3205,   7.0334,\n",
            "          0.1217,   0.9271,   0.0646,   3.6638])), (tensor([ 0.4814,  0.0588,  0.0911,  0.0588, -1.5353,  0.0588,  0.4369,  0.0588,\n",
            "         0.1246,  0.0588,  0.6022,  0.0588, -2.5375,  0.0588,  1.3668,  0.0588,\n",
            "         0.1561,  0.0588,  1.2074,  0.0588]), tensor([ -4.0033,  -2.6933, -16.1753,   1.2117,  11.8482,   1.1765,   3.3292,\n",
            "         -1.7279,  -2.5998,  -5.9532, -10.3088,  11.2252, -11.6901,  -4.0053,\n",
            "          6.8551,   2.1233,  -7.3027,   4.2208,   4.6185,  -8.1954,   2.9274,\n",
            "         -7.5653,   3.0321,  -2.0331,   5.5916,   2.4937,  14.4428,   3.6052,\n",
            "         -7.5812,   7.8026, -10.2536,  14.0932])), (tensor([ 1.1296,  0.0553,  0.8246,  0.0553,  0.4529,  0.0553, -0.9485,  0.0553,\n",
            "         0.7104,  0.0553, -0.1970,  0.0553,  0.3377,  0.0553,  0.8334,  0.0553,\n",
            "        -0.7557,  0.0553,  0.9233,  0.0553]), tensor([  5.5686,   7.1311,  -8.8457,  -0.0177,   5.2807,   1.0038,   7.0187,\n",
            "         -0.2721, -11.3679,  -2.5485,  -9.0068,   6.9377,  -9.6427,  -1.9691,\n",
            "          8.6756,   1.5572,  -3.3543,   1.7135,  12.9524,  -9.6623,  11.3801,\n",
            "          5.4841,   1.9546,  -1.5160,  -0.8580,   5.1349,  15.3313,   7.9979,\n",
            "         -2.8763,  10.3228,  -0.6916,  10.3240])), (tensor([ 0.4096,  0.0614,  0.4609,  0.0614,  1.3230,  0.0614, -0.0836,  0.0614,\n",
            "         0.5907,  0.0614, -0.1822,  0.0614, -1.3570,  0.0614, -1.4997,  0.0614,\n",
            "        -0.5275,  0.0614, -0.2082,  0.0614]), tensor([  3.4639,  -4.9488, -13.5088,   2.8396,   8.2714,   1.4933,   2.2161,\n",
            "         -4.6094,  -3.7661,  -3.9009, -11.7520,  14.8056,  -9.0624,  -1.9715,\n",
            "          7.1313,   3.8677,  -9.2555,   4.7172,   4.7240, -10.6461,   0.2806,\n",
            "         -2.1343,   3.1942,   0.0970,   3.8285,   9.1093,  14.8229,   9.2432,\n",
            "         -6.1162,  10.7908,  -6.9091,  14.0331])), (tensor([ 0.9333,  0.0592,  0.3675,  0.0592,  2.1475,  0.0592, -0.3603,  0.0592,\n",
            "         0.3695,  0.0592,  0.2431,  0.0592,  0.0391,  0.0592,  0.3395,  0.0592,\n",
            "        -0.8341,  0.0592,  0.1354,  0.0592]), tensor([ -3.0546,  -9.0305,  -8.2352,   0.4569,   0.1664,  -0.0857,   6.8861,\n",
            "         -6.0151,  -4.7666,  -5.4267,  -0.8754,  12.8080,  -4.6629, -13.4796,\n",
            "          4.9061,   9.0877, -13.1187,   9.1821,  -2.7973,  -5.7231,  -5.4295,\n",
            "         -8.0046,  -2.2734,   7.4552,   4.1634,  -7.4128,  13.3264,   5.4978,\n",
            "         -0.9125,  -6.2442,   0.5573,   9.4079])), (tensor([ 0.0295,  0.1125,  0.5042,  0.1125,  0.4581,  0.1125, -1.3427,  0.1125,\n",
            "         0.3335,  0.1125, -0.0836,  0.1125,  0.5873,  0.1125, -0.4070,  0.1125,\n",
            "         0.6164,  0.1125,  0.6813,  0.1125]), tensor([  5.7747,  -6.7422, -13.5105,  -4.0841,   1.2581,   0.2069,   2.1680,\n",
            "         -8.5045,  -5.0265,  -3.4652, -14.4434,  21.0378,  -8.2606,   2.9291,\n",
            "          3.3965,  11.0722,  -3.5892,   8.2710,   3.3206, -14.9044,   1.9065,\n",
            "         -4.6726,   4.5483,   1.9042,   6.8617,   6.7881,  18.1850,  12.9245,\n",
            "         -2.2337,  11.4703,   3.8563,  12.9327])), (tensor([ 0.8234,  0.1157,  1.4868,  0.1157,  0.1148,  0.1157,  1.0119,  0.1157,\n",
            "        -0.2940,  0.1157,  0.0703,  0.1157,  1.8170,  0.1157,  1.7630,  0.1157,\n",
            "        -1.5991,  0.1157,  0.8279,  0.1157]), tensor([ -3.9029,  -7.1554,  -9.7486,   4.4154,   4.0566,  -0.8216,   5.9098,\n",
            "         -3.2016,   3.6795,  -4.8136,  -1.6947,  11.5451, -10.0410,   1.8439,\n",
            "         -1.0844,   5.2237,  -7.1241,   9.5243,   0.5612,  -5.4705,  -1.7537,\n",
            "        -10.8777,  -1.2335,  -2.1547,   5.6485,   0.4341,  13.8365,   3.9713,\n",
            "         -4.6673,  -0.1367,   2.5525,  10.0538])), (tensor([ 1.5587,  0.0875, -1.6384,  0.0875, -1.0532,  0.0875, -0.4944,  0.0875,\n",
            "         1.5571,  0.0875,  0.7944,  0.0875, -0.6489,  0.0875, -0.5657,  0.0875,\n",
            "        -0.1659,  0.0875, -0.5926,  0.0875]), tensor([  0.4253,   2.9829, -22.1627, -10.4313,  -0.3123, -11.5784,   6.8409,\n",
            "         -2.6857, -13.3962,  -9.2257,  -9.0303,   5.8867,  -8.8076,   0.0340,\n",
            "          2.5409,   4.9340,   1.1158,   3.2274,   3.3859,  -7.8584,   4.9813,\n",
            "         -0.3431,   8.0897,  -6.3277,  -0.0753,  -5.3200,  17.3021,   8.6406,\n",
            "         -1.1763,   4.7658,  -4.2318,   7.7487])), (tensor([-2.1422,  0.0974, -2.2538,  0.0974, -0.9142,  0.0974,  0.7216,  0.0974,\n",
            "         0.9589,  0.0974, -0.3011,  0.0974, -0.2952,  0.0974,  0.5184,  0.0974,\n",
            "         0.3155,  0.0974,  2.1052,  0.0974]), tensor([  6.5580,   0.1347,  -5.5250,   2.3536,   0.3212,   1.0850,   6.8999,\n",
            "         -4.1247,  -7.5873,  -3.2600,  -5.0489,  12.9324,  -4.6776,  -7.3930,\n",
            "          6.7892,   7.7335, -11.5180,   8.5154,   3.9184,  -7.2615,   1.9366,\n",
            "          5.4481,  -2.4674,   7.4690,   0.3891,   4.1786,  13.5602,  10.6340,\n",
            "         -1.3618,   4.7391,   0.9524,  13.1037])), (tensor([-0.5376,  0.1119, -0.4247,  0.1119,  0.1541,  0.1119, -0.0156,  0.1119,\n",
            "        -0.2740,  0.1119,  0.3872,  0.1119,  1.3892,  0.1119,  0.3782,  0.1119,\n",
            "        -2.1715,  0.1119, -1.8813,  0.1119]), tensor([ -4.9946,  -5.7045, -12.5949,  -2.8661,   7.3408,  -4.4278,   6.3837,\n",
            "        -10.7265,  -6.7879,  -9.6190,  -9.3734,  15.6780,  -7.2207,  -4.6121,\n",
            "          1.7355,  13.5058,  -8.0812,  13.0616,   3.9691, -12.0760,  -4.1064,\n",
            "         -7.3390,   5.1374,   8.1238,   8.5761,  -3.0049,  14.2910,   4.2421,\n",
            "         -2.4376,  -0.4730,  -1.8144,   9.4210])), (tensor([-0.3341,  0.1108, -1.8074,  0.1108,  0.0247,  0.1108,  2.6705,  0.1108,\n",
            "        -0.0294,  0.1108,  0.5612,  0.1108,  0.8197,  0.1108, -0.8027,  0.1108,\n",
            "         1.8565,  0.1108, -0.1922,  0.1108]), tensor([ -0.1312,  -7.6140,  -9.5532,   7.0109,  -1.8258,  -6.6183,  13.1668,\n",
            "         -4.5665,  -4.4274,  -6.1534,   5.1041,   7.4457,  -9.9771,  -2.1471,\n",
            "         -0.1288,   4.6095, -11.9617,   9.5257,   3.4850,  -5.7160,  -4.8115,\n",
            "         -6.2791,  -1.8187,  -3.9518,  -0.9505,  -3.6007,  19.0487,   8.1886,\n",
            "         -1.5970,  -8.3853,   8.0841,   5.7263])), (tensor([-0.0128,  0.1157, -1.5807,  0.1157, -0.9893,  0.1157, -0.2774,  0.1157,\n",
            "         0.2799,  0.1157,  0.2877,  0.1157, -0.5722,  0.1157, -1.2411,  0.1157,\n",
            "         0.7922,  0.1157,  0.1292,  0.1157]), tensor([ -1.8624,  -3.2989, -14.9086,   3.4015,   6.2446,  -5.6362,  12.0474,\n",
            "         -7.2444,  -8.7550, -10.7943,  -4.4471,  14.0999, -10.0195,  -9.2158,\n",
            "          5.0614,  10.3330, -16.2328,  14.1790,   4.5385,  -8.9456,  -3.2899,\n",
            "         -1.9257,   0.5422,   6.0738,   3.2012,  -1.8062,  19.5657,   8.3626,\n",
            "         -4.2957,  -1.6089,  -4.0243,  15.0824])), (tensor([ 1.9259,  0.1157,  1.4228,  0.1157,  0.9987,  0.1157, -0.1231,  0.1157,\n",
            "        -0.3406,  0.1157, -0.5381,  0.1157, -0.2648,  0.1157, -1.3180,  0.1157,\n",
            "         0.0451,  0.1157,  1.0015,  0.1157]), tensor([  5.5701,  -8.6282,  -8.4588,  -0.2786,   2.5360,   2.3118,   4.5576,\n",
            "         -9.2590,  -8.9235,  -1.1894, -11.1354,  16.8261,  -8.8660,  -0.4858,\n",
            "          6.2067,   7.7262,  -4.9568,   4.1428,   9.6989, -17.2043,   1.1934,\n",
            "         -6.2975,   4.8448,   0.3970,   4.6545,   4.8443,  18.7869,  10.9656,\n",
            "         -0.9099,   6.0128,   7.9948,   6.1000])), (tensor([ 0.2678,  0.0739,  1.8771,  0.0739, -1.2630,  0.0739,  0.6614,  0.0739,\n",
            "        -0.4017,  0.0739,  0.1461,  0.0739, -1.0764,  0.0739, -0.4756,  0.0739,\n",
            "        -0.4791,  0.0739, -1.1943,  0.0739]), tensor([  4.5511,  -3.4025, -14.5413,  -3.0886,   5.8182,   3.9606,   1.1118,\n",
            "         -2.3023, -11.7532,  -1.3468, -13.3006,  11.0803,  -7.9215, -10.1865,\n",
            "         13.7594,   0.2777,  -6.2371,  -3.8565,   6.0236, -11.5873,   5.0081,\n",
            "         -2.0386,   4.7218,  -1.6607,   1.1057,   2.9020,  15.4468,   9.2882,\n",
            "         -3.6314,  10.5313,  -8.0587,  10.5413])), (tensor([-0.1628,  0.1157, -0.6646,  0.1157, -0.4688,  0.1157, -1.6661,  0.1157,\n",
            "        -0.9620,  0.1157,  0.1341,  0.1157,  1.4579,  0.1157, -0.9079,  0.1157,\n",
            "        -1.1990,  0.1157,  0.5211,  0.1157]), tensor([ 1.6596, -0.8543, -9.7153, -1.0279, -3.7830, -8.1492,  8.2505, -3.8722,\n",
            "        -1.8698, -5.7321, -1.0049,  8.4172, -7.7579,  8.8212, -5.3834,  7.5952,\n",
            "        -0.7108, 10.3889,  2.3146, -5.5970,  1.2660, -2.3968,  1.1231, -3.9016,\n",
            "         1.6943,  0.1015, 14.4199,  7.8609, -0.3855,  0.5146,  9.3979,  6.1150])), (tensor([-0.2192,  0.1007, -1.4342,  0.1007, -0.1584,  0.1007, -0.6690,  0.1007,\n",
            "         0.0954,  0.1007,  0.1205,  0.1007, -1.0195,  0.1007,  1.0641,  0.1007,\n",
            "         0.6816,  0.1007,  0.2816,  0.1007]), tensor([  2.0906,  -6.4159,  -4.3459,   5.3504,   6.1457,  -1.7081,   5.9234,\n",
            "         -9.0734, -10.9176,  -5.2509,  -3.5962,   8.2144,  -1.5399, -13.5796,\n",
            "          8.1292,   5.9540, -16.1865,   5.6333,   5.5262,  -8.6697,  -9.2568,\n",
            "          2.9932,   2.4801,   9.7276,  -0.0503,   1.5895,   8.7249,   5.7476,\n",
            "         -1.2873,  -3.7324,  -5.8802,   4.4522])), (tensor([-0.0485,  0.0826, -1.5491,  0.0826, -1.1278,  0.0826,  0.9314,  0.0826,\n",
            "         0.3168,  0.0826, -1.0089,  0.0826,  0.6975,  0.0826,  0.7339,  0.0826,\n",
            "        -1.1481,  0.0826, -0.6165,  0.0826]), tensor([ -0.8974,  -1.7591, -21.3015,  -5.2310,  -2.5363, -10.2097,  10.8989,\n",
            "         -3.5402,  -8.6735, -10.3341,  -3.9590,  11.6310, -10.6624,  -2.9864,\n",
            "          1.2150,   8.8010,  -6.6849,  10.6276,  -0.8258,  -6.3840,   1.4716,\n",
            "         -4.3926,   1.9654,  -2.9323,   1.5706,  -6.9760,  21.7664,  10.7123,\n",
            "         -2.0420,  -0.0955,   0.2168,  13.4646])), (tensor([ 0.6102,  0.1059,  1.8403,  0.1059, -0.0762,  0.1059,  0.9181,  0.1059,\n",
            "        -1.6728,  0.1059,  0.6104,  0.1059,  0.7065,  0.1059,  0.6989,  0.1059,\n",
            "        -0.2215,  0.1059, -0.0128,  0.1059]), tensor([  4.5525,   2.6201, -16.5076,  -8.1227,   0.4587,  -8.5269,   5.3888,\n",
            "         -6.0993, -15.9445,  -8.7352, -10.5614,   9.5613,  -3.0101,  -8.2548,\n",
            "          6.6142,   8.7345,  -6.5564,   5.7975,   2.2382,  -8.3203,   0.3798,\n",
            "          7.7118,   6.2511,   5.4300,  -0.5394,  -1.3159,  13.1801,  10.6364,\n",
            "         -0.2266,   5.3797,  -8.3067,  10.3123])), (tensor([ 1.0769,  0.0752,  1.0275,  0.0752, -0.1882,  0.0752,  1.4148,  0.0752,\n",
            "        -1.5908,  0.0752,  0.6039,  0.0752, -0.0071,  0.0752, -0.2070,  0.0752,\n",
            "         0.0497,  0.0752,  0.3216,  0.0752]), tensor([ 1.3915, -2.5834, -6.7638, -3.2199, -4.3427, -6.2985,  6.1397, -6.6545,\n",
            "        -3.2320, -5.7924, -2.2562, 11.1287, -3.0642,  2.0484, -3.6194, 11.5938,\n",
            "        -3.4600, 11.9311, -1.0441, -5.6739, -2.2504, -1.0841,  0.6334,  4.6121,\n",
            "         3.3659, -1.8689, 10.7207,  7.2851,  1.2899, -1.2195,  7.2394,  6.3312])), (tensor([ 0.1424,  0.0848, -0.2828,  0.0848,  0.5956,  0.0848,  0.2904,  0.0848,\n",
            "         0.1978,  0.0848, -1.0029,  0.0848,  0.4437,  0.0848, -1.4764,  0.0848,\n",
            "        -2.1020,  0.0848, -0.9558,  0.0848]), tensor([  3.0986,  -3.1813,  -7.9857,  -0.4006,   2.3407,   0.9914,   4.6976,\n",
            "         -5.2062, -11.2196,  -4.4491,  -5.9299,  11.1228,  -1.8864, -17.1794,\n",
            "         10.8530,   6.7726, -14.2443,   4.8978,   0.7386,  -6.6552,  -2.6611,\n",
            "          3.2420,  -0.2504,  10.2493,   0.4586,  -1.4282,  10.8243,   8.1059,\n",
            "         -1.0793,   0.8891,  -7.1187,  11.0036])), (tensor([-1.7560,  0.0456,  0.8569,  0.0456, -0.0957,  0.0456, -0.1548,  0.0456,\n",
            "         2.3401,  0.0456, -0.4732,  0.0456, -0.2734,  0.0456,  0.8837,  0.0456,\n",
            "         1.0039,  0.0456,  0.8553,  0.0456]), tensor([ -3.5624,  -3.3176, -19.7998,  -5.5498,   2.8719,  -6.0497,   6.6283,\n",
            "         -3.2534,  -6.6074,  -7.6573,  -7.6623,  10.1178, -11.8527,   0.0510,\n",
            "          2.0727,   5.2448,  -1.5904,   5.2728,   2.8587,  -9.0105,   3.2624,\n",
            "         -9.8118,   5.1782,  -6.2987,   4.2553,  -5.0878,  18.8844,   6.3249,\n",
            "         -3.2986,   2.7513,  -0.9945,   9.3212])), (tensor([-2.1207,  0.0990, -0.7961,  0.0990, -1.4725,  0.0990, -0.3017,  0.0990,\n",
            "        -0.4865,  0.0990,  0.0066,  0.0990, -1.5170,  0.0990, -1.3195,  0.0990,\n",
            "         0.4086,  0.0990, -0.9094,  0.0990]), tensor([  4.1424,  -1.5884, -11.3925,   3.6763,   3.2119,  -5.9931,   7.4355,\n",
            "         -6.7626,  -1.6113,  -7.3025,  -6.7787,  14.9200,  -8.8326,   7.3500,\n",
            "         -1.9802,   9.7558,  -7.8428,  14.1031,   4.6554,  -9.0323,  -1.0911,\n",
            "          1.6725,   1.7444,   1.3699,   3.4949,   9.1229,  15.8291,  10.6445,\n",
            "         -4.0607,   6.9150,   2.1239,  13.3940])), (tensor([-1.5605,  0.1233,  1.5354,  0.1233,  0.3591,  0.1233, -0.6381,  0.1233,\n",
            "         1.4751,  0.1233, -1.0541,  0.1233,  0.3201,  0.1233, -2.0769,  0.1233,\n",
            "        -0.7122,  0.1233,  0.4308,  0.1233]), tensor([  5.8403,  -2.4683, -10.1653,  -3.7598,  -7.7581,  -3.4914,   4.5866,\n",
            "         -0.4760,  -6.1339,  -1.5169,  -1.5655,   6.5660,  -4.4434,  -1.5907,\n",
            "          2.5046,   2.5266,  -2.2496,   1.2925,  -1.7922,  -3.7187,   2.4853,\n",
            "         -0.3365,  -0.1617,  -4.5607,  -2.3150,  -2.0114,  13.1929,  10.4641,\n",
            "          0.9535,   2.0562,   4.7798,   6.0250])), (tensor([-0.4074,  0.0754, -0.8267,  0.0754,  0.3634,  0.0754,  1.3257,  0.0754,\n",
            "         0.5714,  0.0754,  0.4037,  0.0754, -0.3950,  0.0754, -0.2649,  0.0754,\n",
            "         0.2417,  0.0754,  1.2743,  0.0754]), tensor([ -3.2166,   3.8411, -20.7588,  -4.9314,  -0.4901, -13.8425,  12.2151,\n",
            "         -1.2225, -10.1040, -12.0127,  -1.3208,   4.0129, -10.1286,  -1.9343,\n",
            "         -0.1575,   5.6642,  -4.7002,   9.1300,   1.5560,  -3.0763,   2.5170,\n",
            "         -0.6331,   3.0595,  -4.8703,  -1.0041,  -8.6361,  18.1325,   6.8224,\n",
            "         -2.1892,  -2.2253,  -3.3010,  10.1626])), (tensor([-1.3020,  0.1111,  1.4193,  0.1111,  1.0250,  0.1111, -0.9297,  0.1111,\n",
            "         0.1323,  0.1111,  0.5172,  0.1111,  1.3767,  0.1111, -0.4130,  0.1111,\n",
            "        -0.0837,  0.1111,  1.3487,  0.1111]), tensor([ -0.0399,  -4.8586, -17.8895,   2.3000,   3.2571,  -5.2169,  10.5800,\n",
            "         -3.8180,  -9.8779,  -8.5332,  -3.3196,  10.7550, -10.3574, -10.9763,\n",
            "          7.6238,   4.7254, -14.8906,   7.3779,   2.3289,  -7.1782,  -2.1789,\n",
            "         -3.0639,   0.7079,  -0.4963,  -0.1552,  -3.2361,  20.5420,   9.9151,\n",
            "         -4.1936,  -1.0911,  -5.5952,  13.5677])), (tensor([ 0.0711,  0.0904,  0.7405,  0.0904, -0.4050,  0.0904,  0.8470,  0.0904,\n",
            "        -0.9521,  0.0904,  1.5996,  0.0904,  2.1920,  0.0904, -0.3657,  0.0904,\n",
            "        -0.0065,  0.0904, -0.9689,  0.0904]), tensor([  1.1960,  -3.3976, -13.9549,   0.8270,   1.8322,  -2.1455,  12.1280,\n",
            "         -5.1603, -12.1647,  -6.9245,  -4.4610,  13.5668, -10.9100, -11.1506,\n",
            "          8.4718,   7.9086, -13.0859,   8.7910,   6.1766, -10.5916,   1.9824,\n",
            "         -3.4087,  -0.2712,   2.4947,   1.3657,  -4.2143,  22.7448,  10.5925,\n",
            "         -2.1965,  -0.7433,   1.6998,  13.1362])), (tensor([-0.0285,  0.1137,  0.3801,  0.1137, -0.0306,  0.1137,  1.4351,  0.1137,\n",
            "        -0.1839,  0.1137, -0.2114,  0.1137, -1.5146,  0.1137,  1.5822,  0.1137,\n",
            "         0.7287,  0.1137,  1.5931,  0.1137]), tensor([  1.8196,  -1.0031,  -9.3877,  -6.7936,   3.3852,  -9.1103,   3.2622,\n",
            "        -10.5540, -12.2916,  -6.3154, -10.7578,   6.4845,  -3.3768,   5.3462,\n",
            "         -0.7728,   8.3587,   2.8335,   4.1778,   9.8114, -13.4518,  -1.8619,\n",
            "          0.8950,  11.8306,   0.8675,   2.8859,   1.4412,   9.0850,   4.9620,\n",
            "          1.4061,   2.7977,   1.8740,  -2.6244])), (tensor([-0.1625,  0.0505,  1.4798,  0.0505, -1.6825,  0.0505,  0.7690,  0.0505,\n",
            "        -0.2231,  0.0505, -1.9729,  0.0505,  0.9675,  0.0505,  1.5370,  0.0505,\n",
            "         2.1830,  0.0505,  0.5735,  0.0505]), tensor([ 0.7754, -4.4470, -4.9250,  5.4002, 10.3451, -0.0427,  2.9023, -6.6222,\n",
            "        -7.0980, -3.4217, -6.6525,  5.8492, -4.4950, -5.0004,  6.4064,  1.6700,\n",
            "        -9.2484,  1.7750,  9.2360, -9.5801, -4.7074,  0.1142,  5.0300,  2.9065,\n",
            "         1.4262,  5.7997,  7.2108,  2.8208, -3.4295,  1.8357, -6.3590,  2.6599])), (tensor([-0.2271,  0.0280, -0.5629,  0.0280, -2.4936,  0.0280,  1.0128,  0.0280,\n",
            "        -1.1344,  0.0280, -0.2314,  0.0280,  3.2813,  0.0280,  0.3181,  0.0280,\n",
            "        -2.7974,  0.0280, -0.3506,  0.0280]), tensor([-1.4868, -0.7645, -9.6407,  2.1850,  3.7002, -5.4608, 10.8411, -5.5362,\n",
            "        -6.9532, -7.7226, -2.3210,  8.9916, -9.1240, -1.7801,  1.1802,  7.8440,\n",
            "        -8.1913, 10.8933,  7.1442, -8.1074,  0.1050, -2.3036,  1.1928,  1.7445,\n",
            "         2.3789, -1.7939, 15.9524,  5.6075, -2.1491, -2.0095,  2.9622,  8.1260])), (tensor([-0.2205,  0.1157, -1.0871,  0.1157,  0.2233,  0.1157, -1.2009,  0.1157,\n",
            "         1.4685,  0.1157, -0.0233,  0.1157,  0.1389,  0.1157, -2.4848,  0.1157,\n",
            "         1.3416,  0.1157, -0.0202,  0.1157]), tensor([ 0.5206, -3.2992, -1.8806,  5.7954,  2.2728,  6.7374,  7.0309, -1.2843,\n",
            "         0.2030, -0.2517, -1.1742, 12.1427, -8.9592, -4.2260,  4.6316,  4.7565,\n",
            "        -8.5986,  7.4077,  4.7993, -6.4516,  4.6395, -6.2992, -5.7296,  2.4283,\n",
            "         3.8996,  1.2205, 14.1430,  5.2919, -2.7652,  1.4903,  7.5326, 11.0803])), (tensor([-0.3522,  0.0642,  0.0822,  0.0642,  1.1696,  0.0642,  0.2445,  0.0642,\n",
            "         1.9150,  0.0642, -1.2912,  0.0642,  0.0332,  0.0642,  0.6362,  0.0642,\n",
            "        -0.0322,  0.0642,  1.1134,  0.0642]), tensor([  1.8496,  -2.5159, -11.0636,  -7.8130,   0.9511,  -2.7240,   3.8029,\n",
            "         -7.3620, -10.4712,  -3.8238, -11.2379,  11.7105,  -6.9040,   1.2333,\n",
            "          2.9213,   8.4080,   1.9697,   3.7384,   7.3760, -13.7439,   4.0117,\n",
            "         -4.7774,   7.1795,  -0.1963,   4.6733,  -1.2671,  14.8502,   7.2954,\n",
            "          0.6828,   5.0782,   5.3571,   3.7414])), (tensor([ 0.1367,  0.1019,  0.6693,  0.1019,  0.4067,  0.1019, -1.6524,  0.1019,\n",
            "         0.4683,  0.1019, -1.5656,  0.1019,  0.7956,  0.1019,  0.4417,  0.1019,\n",
            "        -0.9139,  0.1019, -0.1047,  0.1019]), tensor([  1.6273,  -0.5085,  -9.0660,  -2.4126,   5.1357,   0.4302,   5.3705,\n",
            "         -7.6380,  -7.5076,  -5.9395, -11.2824,  17.0841,  -6.4769,  -4.6163,\n",
            "          5.1005,  12.3169,  -7.5690,  11.3501,   5.5770, -11.7325,   2.5951,\n",
            "         -0.1278,   1.9490,   9.4838,   6.4176,   2.6086,  14.2862,   7.8860,\n",
            "         -2.1469,   6.7507,  -0.2789,  13.5584])), (tensor([-0.4368,  0.0978, -0.5288,  0.0978,  0.0562,  0.0978,  0.5790,  0.0978,\n",
            "         1.7191,  0.0978,  0.1073,  0.0978, -0.5969,  0.0978, -0.5133,  0.0978,\n",
            "        -0.4149,  0.0978,  1.2858,  0.0978]), tensor([  3.6599,  -8.6396, -11.8683,  -4.4047,  -3.7640,  -7.3690,   5.2159,\n",
            "         -8.9583,  -9.3524,  -4.4705,  -5.6489,  10.3399,  -5.2961,   1.7147,\n",
            "          0.1559,   7.5554,  -2.1136,   4.3147,   3.0400, -11.6765,  -4.1685,\n",
            "         -5.0823,   6.7114,  -2.8160,   1.4983,  -1.6392,  15.5075,  10.0545,\n",
            "          1.6102,  -0.8791,   7.1487,   0.7658])), (tensor([ 0.1244,  0.0737,  0.5937,  0.0737,  0.4184,  0.0737,  1.9436,  0.0737,\n",
            "         0.1412,  0.0737,  0.7852,  0.0737,  1.2118,  0.0737, -1.1811,  0.0737,\n",
            "        -0.6635,  0.0737,  0.7222,  0.0737]), tensor([ -0.9866,   2.9815,  -9.3041,   7.9878,   3.2619,  -3.7897,  13.7888,\n",
            "          1.5387,  -4.2391,  -6.3320,   3.8121,   3.8106, -12.0856,  -3.0984,\n",
            "          2.8920,   1.0467, -11.0872,   8.5803,   7.0854,  -2.2844,   4.1795,\n",
            "         -0.2979,  -4.3367,  -3.8893,  -2.0924,  -1.3132,  17.2959,   5.4745,\n",
            "         -4.4272,  -2.3659,   1.5844,  11.3853])), (tensor([ 1.0106,  0.1157,  0.9816,  0.1157, -0.3176,  0.1157,  1.5760,  0.1157,\n",
            "        -0.0218,  0.1157,  1.4176,  0.1157, -0.6569,  0.1157,  0.4026,  0.1157,\n",
            "        -0.1732,  0.1157,  0.3702,  0.1157]), tensor([ -2.4897,  -0.2642,  -9.3269,   4.5964,   5.1120,   3.4503,   7.9056,\n",
            "          1.4513,  -2.2495,  -4.6325,  -1.2049,   9.7200,  -9.0599, -12.1821,\n",
            "          7.9784,   3.0841, -13.4940,   7.4254,   0.7437,  -2.0805,   3.6311,\n",
            "         -3.3203,  -5.8808,   3.4266,   2.0756,  -2.3365,  14.0274,   4.5966,\n",
            "         -5.2426,   1.1282,  -5.2296,  16.6043])), (tensor([ 0.6861,  0.0794, -0.3207,  0.0794, -0.4721,  0.0794,  0.0995,  0.0794,\n",
            "         1.3899,  0.0794,  1.6639,  0.0794,  1.1838,  0.0794,  0.0827,  0.0794,\n",
            "        -1.3184,  0.0794,  0.3777,  0.0794]), tensor([  2.8399,  -1.2518,  -6.0447,  -2.8360,   3.7227,   5.3289,   0.8838,\n",
            "         -5.1482,  -0.3355,  -1.4812, -12.7186,  18.6431,  -6.9138,   2.9674,\n",
            "          2.6017,  10.5232,  -1.3713,   9.1052,   4.0233, -11.2306,   6.5391,\n",
            "         -3.6898,   0.5711,   5.7921,   8.6698,   6.7841,  12.0574,   7.3134,\n",
            "         -2.5744,  12.1572,   4.2922,  13.5484])), (tensor([ 0.9285,  0.1039, -0.0876,  0.1039,  0.4821,  0.1039,  1.8560,  0.1039,\n",
            "        -1.2688,  0.1039,  1.0197,  0.1039, -1.1113,  0.1039, -1.1862,  0.1039,\n",
            "        -0.8803,  0.1039, -1.4697,  0.1039]), tensor([  5.9527,   5.2548, -15.2583,  -2.1580,   3.2867,  -5.9331,   8.8209,\n",
            "         -4.1582, -11.8325,  -7.3553, -10.5033,  11.6095,  -9.9836,   1.2456,\n",
            "          4.7457,   7.2700,  -5.1391,   8.3106,   9.5420, -10.8880,   6.8895,\n",
            "          6.2258,   4.1036,  -0.1557,   0.3575,   5.2786,  18.9202,  11.8525,\n",
            "         -2.9313,  10.0575,  -1.2389,  13.6011])), (tensor([ 0.4860,  0.1072,  0.5382,  0.1072, -1.0117,  0.1072,  0.1094,  0.1072,\n",
            "        -0.3508,  0.1072,  0.7571,  0.1072, -1.1815,  0.1072, -0.4276,  0.1072,\n",
            "        -0.7984,  0.1072,  0.2429,  0.1072]), tensor([  0.1984,  -6.8531,  -8.2844,   8.2162,   8.4664,  -0.8347,  10.2949,\n",
            "         -8.1414,  -8.5386,  -6.1181,  -3.9406,  11.9664,  -9.7462,  -8.1036,\n",
            "          7.0511,   6.1112, -15.8119,   8.7655,  10.0723, -11.9305,  -4.3683,\n",
            "         -3.5331,   1.3743,   3.8684,   2.2824,   2.3551,  17.6397,   7.0157,\n",
            "         -3.9460,  -2.0498,  -0.3239,   8.6861])), (tensor([-0.6271,  0.0863, -0.7294,  0.0863,  1.3465,  0.0863,  0.3318,  0.0863,\n",
            "        -0.5150,  0.0863, -0.7719,  0.0863,  0.1248,  0.0863, -1.5279,  0.0863,\n",
            "        -0.4879,  0.0863,  0.8848,  0.0863]), tensor([  3.3399,  -6.4071,  -3.6164,   2.3041,   7.3136,   6.9247,  -0.4120,\n",
            "         -5.3891,  -5.5169,   0.9605, -10.0357,  11.1342,  -4.6378,  -6.1413,\n",
            "          9.2239,   2.2970,  -6.5797,  -0.7137,   7.1837, -11.4809,   0.0819,\n",
            "         -3.3391,   2.9878,   3.3090,   3.6575,   6.3775,   8.8107,   5.1474,\n",
            "         -2.6424,   6.7264,  -2.2011,   5.3832])), (tensor([ 1.0140,  0.0895,  1.3484,  0.0895, -1.4184,  0.0895, -0.1591,  0.0895,\n",
            "         0.2080,  0.0895,  1.8299,  0.0895, -1.4455,  0.0895,  0.0992,  0.0895,\n",
            "        -0.4895,  0.0895, -0.3784,  0.0895]), tensor([ 1.4139, -8.7942, -8.5563,  2.2735, -3.5080, -9.0414,  7.5871, -8.6934,\n",
            "        -1.8434, -6.9240,  0.7343, 10.9373, -4.1159,  2.5747, -4.6691, 10.0015,\n",
            "        -8.6247, 12.5096, -1.7294, -5.9519, -9.5078, -3.2999,  1.2781,  1.7404,\n",
            "         1.9694, -0.0777, 12.6077,  8.6215, -0.0350, -5.1471,  6.3562,  4.8332])), (tensor([-0.3179,  0.1195, -1.1578,  0.1195,  0.4724,  0.1195,  0.6377,  0.1195,\n",
            "        -0.3861,  0.1195, -0.1483,  0.1195, -0.9560,  0.1195, -0.7505,  0.1195,\n",
            "        -0.0986,  0.1195, -1.6496,  0.1195]), tensor([ -4.6031,   6.0704, -11.3815,   7.4925,  10.3156,  -3.4770,  13.8604,\n",
            "          0.9829,  -5.5718,  -8.6973,  -0.1971,   4.0425, -14.1143,  -3.8433,\n",
            "          4.3896,   1.6519, -10.9212,   9.5995,  11.0850,  -4.2186,   6.1797,\n",
            "         -0.5962,  -1.9992,  -2.0778,   0.3899,  -0.8717,  17.0960,   2.3731,\n",
            "         -6.5931,  -0.0321,  -3.9301,  13.2612])), (tensor([-0.7245,  0.0439,  1.4444,  0.0439,  0.8148,  0.0439, -0.6508,  0.0439,\n",
            "        -0.6691,  0.0439,  0.3862,  0.0439, -1.1587,  0.0439,  0.9656,  0.0439,\n",
            "        -0.2303,  0.0439, -0.9173,  0.0439]), tensor([  3.4268,  -3.1403,  -9.1674,   5.9969,  -0.7126,  -4.6802,  11.9487,\n",
            "         -2.3412, -10.7692,  -4.8044,   2.9929,   4.0027,  -7.6787,  -9.3976,\n",
            "          6.5819,   0.9729, -13.5715,   3.8904,   5.5906,  -5.0847,  -2.0204,\n",
            "          1.2858,  -1.3112,  -2.2728,  -4.8986,  -2.5053,  17.0315,   9.1093,\n",
            "         -1.3324,  -5.2358,   1.3065,   6.1251])), (tensor([-1.2557,  0.1001, -0.8660,  0.1001, -0.8658,  0.1001, -1.1663,  0.1001,\n",
            "         0.2672,  0.1001,  1.4167,  0.1001, -0.0136,  0.1001, -0.0981,  0.1001,\n",
            "         0.2017,  0.1001,  1.5390,  0.1001]), tensor([  6.0584,  -4.5179, -13.1389,  -0.8259,   2.7511,  -5.6724,   5.6075,\n",
            "         -8.7123, -12.5389,  -5.8911,  -9.6369,  11.6149,  -5.8627,  -3.0945,\n",
            "          5.7509,   6.7127,  -8.3303,   4.9068,   6.8025, -12.6480,  -3.1289,\n",
            "          2.5241,   6.7248,   1.4087,   0.2519,   4.5064,  15.7439,  11.4931,\n",
            "         -1.4098,   4.6080,  -2.2261,   6.8724])), (tensor([ 0.2773,  0.0477, -0.2269,  0.0477, -0.4631,  0.0477, -2.6965,  0.0477,\n",
            "        -1.7746,  0.0477, -1.6451,  0.0477, -0.5050,  0.0477,  1.0459,  0.0477,\n",
            "         1.4052,  0.0477,  0.6808,  0.0477]), tensor([ -3.6927,   2.0994, -14.4486,  -4.5304,   2.2930, -10.3430,   9.5984,\n",
            "         -6.7798,  -6.5684, -12.0974,  -5.1662,  11.5512,  -6.7472,  -1.0436,\n",
            "         -2.2393,  13.6088,  -6.2517,  16.1532,   1.1816,  -6.1180,  -0.7376,\n",
            "         -0.2905,   2.6267,   6.3532,   4.9306,  -4.5711,  14.0085,   5.5750,\n",
            "         -1.6996,  -0.5867,  -1.4536,  12.0279])), (tensor([-0.2972,  0.0973, -1.1120,  0.0973, -0.4626,  0.0973,  0.2991,  0.0973,\n",
            "         0.3564,  0.0973, -0.3384,  0.0973, -0.7369,  0.0973,  1.4016,  0.0973,\n",
            "         0.7649,  0.0973,  0.8091,  0.0973]), tensor([  3.3922,  -3.7193,  -4.1199,   4.8514,   6.8037,   1.3185,   9.2797,\n",
            "        -10.0124,  -9.7211,  -4.1229,  -7.9046,  15.2297,  -8.9084,  -3.1046,\n",
            "          5.8199,  10.0059, -10.3880,  10.1385,  13.8415, -15.8095,   0.5069,\n",
            "         -1.2302,   2.2858,   6.7394,   4.3376,   5.1519,  17.4558,   8.2433,\n",
            "         -1.7288,   2.4349,   6.7053,   7.8273])), (tensor([-0.6611,  0.0845, -1.3940,  0.0845,  0.4511,  0.0845,  1.1444,  0.0845,\n",
            "         0.6885,  0.0845, -0.6244,  0.0845, -0.3215,  0.0845,  0.1922,  0.0845,\n",
            "         1.1017,  0.0845, -1.1994,  0.0845]), tensor([  8.2053,  -8.8975, -12.4838,  -7.6559,  -3.8152,  -2.3458,   0.3643,\n",
            "         -7.8504, -14.8873,  -1.2625, -10.7433,  10.8376,  -1.7249,  -7.9259,\n",
            "          8.8399,   4.6099,  -3.7262,  -3.3467,   1.6305, -12.6139,  -2.8582,\n",
            "         -0.8191,   7.7459,  -0.1417,  -0.6605,  -0.3206,  13.4493,  12.5018,\n",
            "          2.1447,   4.2175,  -0.3670,   1.9709])), (tensor([-0.3449,  0.0760,  1.3036,  0.0760, -1.3129,  0.0760, -0.4884,  0.0760,\n",
            "         0.5218,  0.0760, -0.2883,  0.0760, -1.5771,  0.0760, -0.2284,  0.0760,\n",
            "         0.9863,  0.0760, -0.2038,  0.0760]), tensor([ -2.9101,  -2.9977,  -9.2595,   3.3519,   5.2307,  -6.2614,   8.8856,\n",
            "         -4.1785, -10.8863,  -7.0875,   0.8432,   0.6154,  -5.3098, -10.8844,\n",
            "          5.6114,   1.1728, -11.3183,   2.8364,   5.6751,  -4.7834,  -5.4176,\n",
            "         -0.9466,   2.9504,   0.4770,  -2.1394,  -5.4619,  10.5509,   2.3699,\n",
            "         -1.7798,  -7.5280,  -5.8417,   1.8193])), (tensor([ 0.0092,  0.0792, -0.7995,  0.0792, -0.6226,  0.0792,  1.4007,  0.0792,\n",
            "        -0.8585,  0.0792,  0.2226,  0.0792,  1.2415,  0.0792, -0.8144,  0.0792,\n",
            "        -0.2492,  0.0792,  1.4118,  0.0792]), tensor([ -5.9373,  -0.9930, -12.5267,   3.2801,   5.5012,  -7.3429,  12.3200,\n",
            "         -2.3781,  -5.4828,  -7.9357,   0.8219,   2.9710, -13.2993,   1.8166,\n",
            "         -0.4901,   2.2113,  -4.5494,   6.9820,   9.3667,  -6.7544,   1.7934,\n",
            "         -8.3717,   2.4991,  -7.6135,   1.4092,  -5.2516,  17.4537,   1.9928,\n",
            "         -3.5080,  -5.0032,   3.4642,   4.1163])), (tensor([-1.5452,  0.0933,  0.9662,  0.0933, -0.6919,  0.0933,  0.3699,  0.0933,\n",
            "        -1.3593,  0.0933, -1.7925,  0.0933,  1.3765,  0.0933, -0.2262,  0.0933,\n",
            "         0.8963,  0.0933,  1.5602,  0.0933]), tensor([ 5.9737e+00, -7.4817e+00, -1.3814e+01, -1.2085e+00, -2.1694e+00,\n",
            "         3.3732e+00,  5.0697e+00, -2.1687e+00, -9.5706e+00, -1.1928e+00,\n",
            "        -6.7260e+00,  1.4373e+01, -8.0553e+00, -1.2587e+01,  1.1896e+01,\n",
            "         3.1522e+00, -1.0845e+01,  6.7624e-01,  2.8665e-01, -9.1250e+00,\n",
            "         2.5836e+00, -4.3067e+00, -9.8947e-01, -8.7704e-01, -3.0713e-03,\n",
            "        -1.0758e+00,  1.9981e+01,  1.3327e+01, -1.9090e+00,  4.7591e+00,\n",
            "        -2.9839e-01,  1.3183e+01])), (tensor([ 1.0131,  0.0928, -0.2724,  0.0928,  0.3742,  0.0928,  1.0248,  0.0928,\n",
            "        -0.7181,  0.0928, -0.9942,  0.0928,  1.4057,  0.0928, -0.0135,  0.0928,\n",
            "         0.9129,  0.0928,  0.2908,  0.0928]), tensor([  4.8251,  -4.5191,  -6.7381,   5.0493,   3.0008,   6.7224,   7.1956,\n",
            "         -0.8141, -11.3159,   0.4928,  -3.4223,   8.5277,  -8.3630, -16.0061,\n",
            "         14.9357,  -1.1221, -13.4372,  -2.0199,   7.6776,  -8.7945,   3.7215,\n",
            "         -1.8542,  -2.3979,   0.0810,  -2.1919,  -0.1134,  17.1961,   8.9747,\n",
            "         -2.4637,   1.6769,  -0.7691,   9.2741])), (tensor([ 1.2652,  0.0501,  1.4157,  0.0501,  0.5710,  0.0501,  0.5089,  0.0501,\n",
            "         0.4623,  0.0501, -0.4266,  0.0501, -1.0453,  0.0501,  0.1884,  0.0501,\n",
            "        -1.4570,  0.0501, -0.1251,  0.0501]), tensor([  3.6798,  -2.7196, -19.2183,  -2.9370,  -0.8006,  -3.9800,  10.4694,\n",
            "         -3.9767, -12.9975,  -7.2900,  -7.1792,  14.8037, -10.9168,  -9.8074,\n",
            "          8.8105,   7.5707, -11.1487,   7.3801,   3.0100, -10.1517,   3.4732,\n",
            "         -1.8173,   0.9875,  -0.1058,   0.4583,  -3.3631,  24.2837,  13.8995,\n",
            "         -2.3556,   3.5071,  -0.7978,  15.8347])), (tensor([ 0.0248,  0.0659, -0.0850,  0.0659,  1.1448,  0.0659, -1.0257,  0.0659,\n",
            "         0.3727,  0.0659,  0.8236,  0.0659,  0.5207,  0.0659,  1.4085,  0.0659,\n",
            "         1.0597,  0.0659,  0.3743,  0.0659]), tensor([  5.2246,  -0.5123,  -7.7043,   0.4179,   0.9978, -10.1583,   7.9761,\n",
            "         -9.5558, -14.0480,  -8.2334,  -3.8696,   6.2521,  -0.6426,  -5.7992,\n",
            "          2.8998,   8.6857, -10.9288,   8.4697,   5.5952,  -7.9737,  -7.3573,\n",
            "          9.8349,   5.1655,   7.6385,  -2.4548,   1.7753,   9.6118,   8.8239,\n",
            "          0.8969,  -2.1294,  -3.1063,   3.5048])), (tensor([-1.3631,  0.0934, -1.1448,  0.0934, -0.9169,  0.0934,  2.4734,  0.0934,\n",
            "        -1.9204,  0.0934,  1.2949,  0.0934, -0.8482,  0.0934, -0.5514,  0.0934,\n",
            "         0.3115,  0.0934, -1.6948,  0.0934]), tensor([ -3.9536,  -7.9675,  -4.9825,   4.3623,   5.2717,   1.2782,   7.7338,\n",
            "         -5.3643,  -5.7381,  -3.1056,  -0.6470,   7.1612,  -8.2546,  -7.6382,\n",
            "          4.9989,   3.3742,  -8.8197,   3.9076,   7.2785,  -9.0696,  -2.4465,\n",
            "        -10.3149,   0.7734,   0.4018,   3.1651,  -4.6355,  13.4806,   1.9118,\n",
            "         -1.7131,  -6.2359,   3.9927,   2.1904])), (tensor([ 0.0228,  0.1157,  0.2009,  0.1157,  0.2058,  0.1157, -0.7364,  0.1157,\n",
            "        -0.0902,  0.1157, -0.6261,  0.1157,  0.6324,  0.1157, -0.6151,  0.1157,\n",
            "        -0.3485,  0.1157, -1.2788,  0.1157]), tensor([ -3.9004,  -9.1255, -17.2374,   0.5189,   3.1425,  -6.2556,   8.8195,\n",
            "         -7.4573,  -4.6692,  -9.9497,  -3.9468,  14.9121,  -9.2694,  -6.8639,\n",
            "          2.1436,  10.0387, -13.0272,  12.4974,  -1.4713,  -7.9426,  -6.3991,\n",
            "         -8.9343,   1.5341,   2.5702,   5.0162,  -4.5898,  18.6197,   7.8092,\n",
            "         -3.7440,  -3.2376,  -2.6514,  12.7834])), (tensor([-0.9527,  0.0326,  0.4179,  0.0326, -1.9883,  0.0326,  0.6485,  0.0326,\n",
            "         0.1804,  0.0326,  0.8000,  0.0326, -0.7357,  0.0326,  0.8425,  0.0326,\n",
            "         0.0259,  0.0326,  1.3714,  0.0326]), tensor([  3.6452,  -0.8586, -16.7317,  -5.1800,  -3.6280,  -4.2837,   5.8482,\n",
            "         -1.9711,  -4.0813,  -5.6691,  -7.1992,  14.5551,  -8.7812,   1.2030,\n",
            "          1.4329,   8.1473,  -3.4812,   8.8020,  -1.9225,  -6.3609,   4.9733,\n",
            "         -2.1059,   0.2181,  -1.9946,   2.6789,   0.1643,  18.3208,  12.1294,\n",
            "         -2.1801,   7.6119,   2.0244,  15.4602])), (tensor([ 1.3768,  0.0924,  0.5483,  0.0924, -0.2455,  0.0924, -0.3509,  0.0924,\n",
            "         0.7360,  0.0924, -0.3057,  0.0924, -0.1481,  0.0924, -1.8255,  0.0924,\n",
            "         0.0518,  0.0924,  0.5184,  0.0924]), tensor([  3.0400,  -9.3073, -13.6136,  -3.8034,   2.0506,   3.0318,   1.3472,\n",
            "         -6.2614,  -7.9925,  -2.5889, -11.6116,  17.0528,  -6.1751, -10.0824,\n",
            "          9.7055,   6.8197,  -8.4637,   2.6191,   0.1419, -11.8064,  -0.6313,\n",
            "         -6.5489,   2.9997,   3.2267,   4.6358,   0.3406,  16.0103,  10.3779,\n",
            "         -2.1894,   6.2413,  -3.1076,  11.6027])), (tensor([ 0.7008,  0.0535, -1.1821,  0.0535,  0.1679,  0.0535,  0.5856,  0.0535,\n",
            "         0.0257,  0.0535,  0.2341,  0.0535,  1.4438,  0.0535,  1.5468,  0.0535,\n",
            "         0.2281,  0.0535,  1.3270,  0.0535]), tensor([  5.7528,  -8.2811,  -9.0537,  -3.1418,  -6.7472,   1.8173,   2.9822,\n",
            "         -2.9708,  -8.1517,  -0.7066,  -3.0499,  11.8180,  -1.7699, -14.3228,\n",
            "          8.9513,   4.8487, -10.2186,   1.2180,  -5.4019,  -5.0692,  -2.0325,\n",
            "         -2.1012,  -2.1096,   3.5579,  -0.7646,  -4.0771,  13.1359,  11.5285,\n",
            "          1.0608,   0.0611,   0.4822,   9.1163])), (tensor([ 0.8598,  0.0454,  1.1844,  0.0454, -0.8560,  0.0454, -0.4183,  0.0454,\n",
            "         1.2515,  0.0454,  1.0884,  0.0454, -1.0847,  0.0454, -0.5427,  0.0454,\n",
            "         1.2855,  0.0454,  1.9680,  0.0454]), tensor([  3.7102,  -5.7304,  -4.8668,   7.5671,   6.0242,  -3.2137,   5.5720,\n",
            "         -7.5650,  -5.7546,  -3.8001,  -3.6379,   7.0515,  -5.2689,   0.0582,\n",
            "          2.7445,   3.1086, -10.0043,   5.0140,   8.6315,  -9.4819,  -6.3615,\n",
            "          1.2651,   3.8301,   0.7766,  -0.0654,   7.6402,  10.2047,   6.3901,\n",
            "         -2.6292,   0.4864,  -0.3442,   2.7049])), (tensor([-0.9190,  0.1157, -0.9038,  0.1157,  1.1392,  0.1157,  0.8890,  0.1157,\n",
            "        -0.8416,  0.1157,  1.0369,  0.1157, -1.0432,  0.1157, -0.4804,  0.1157,\n",
            "         1.7063,  0.1157,  0.2962,  0.1157]), tensor([  1.3837,  -6.2019, -18.8172,  -6.7054,  -4.1915, -11.1833,   7.1046,\n",
            "         -6.1569, -12.9212,  -8.2715,  -3.9844,   7.5267,  -5.0918,  -6.7015,\n",
            "          3.1415,   6.2183,  -6.2678,   3.9449,  -1.6572,  -7.1877,  -4.8569,\n",
            "         -2.6994,   5.9374,  -2.5723,  -1.2066,  -7.5576,  16.5655,  10.3203,\n",
            "          0.6527,  -3.2415,  -2.0855,   4.8625])), (tensor([ 0.3829,  0.0872,  1.1568,  0.0872, -1.3354,  0.0872, -0.3686,  0.0872,\n",
            "         0.3931,  0.0872, -0.7979,  0.0872,  1.7327,  0.0872,  0.1637,  0.0872,\n",
            "         0.4297,  0.0872, -1.0402,  0.0872]), tensor([-0.7036,  1.6249, -2.3564,  2.3370,  6.9727,  3.2997,  1.8503, -2.1955,\n",
            "         2.4400, -2.0937, -6.5570,  9.6536, -5.9791,  3.8338,  0.3601,  5.3170,\n",
            "        -2.1502,  7.5836,  5.2081, -5.7383,  4.2981, -1.6820, -0.4832,  3.5348,\n",
            "         5.8785,  6.4037,  6.1139,  1.5597, -3.7533,  7.3624,  0.5382,  9.2354])), (tensor([ 0.4139,  0.0481,  0.9048,  0.0481, -1.3115,  0.0481,  0.6550,  0.0481,\n",
            "        -0.2571,  0.0481, -1.0379,  0.0481, -2.1484,  0.0481,  0.5172,  0.0481,\n",
            "        -1.1203,  0.0481,  1.1821,  0.0481]), tensor([  8.4607,  -6.5295, -14.3780,  -5.2720,  -3.2056,  -6.5925,   2.9259,\n",
            "         -7.6347, -12.8881,  -3.4682,  -9.5242,  10.0637,  -4.2174,  -0.6362,\n",
            "          4.6809,   4.8115,  -3.0530,   0.1561,   3.4546, -12.0693,  -2.5706,\n",
            "          1.0105,   8.1284,  -3.4018,  -1.2987,   2.7599,  15.2715,  13.3029,\n",
            "          0.8443,   5.1169,   0.9641,   3.1020])), (tensor([ 1.6483,  0.1016, -0.7472,  0.1016, -0.3840,  0.1016,  1.1351,  0.1016,\n",
            "         1.2653,  0.1016,  1.3321,  0.1016,  0.5464,  0.1016, -0.9475,  0.1016,\n",
            "        -1.0140,  0.1016, -0.6916,  0.1016]), tensor([  1.0112,   3.6457, -11.8894,  -1.3726,   4.4881,   0.8014,   7.4149,\n",
            "         -0.8746,  -8.7775,  -5.3593,  -7.4370,  10.8428,  -8.8705,  -8.7128,\n",
            "          8.5248,   5.4867,  -8.2061,   6.4106,   5.2501,  -6.9080,   7.4666,\n",
            "          1.0646,  -0.8250,   3.4280,   1.9614,  -0.6969,  15.8132,   7.2720,\n",
            "         -3.4830,   6.2396,  -4.1221,  15.2594])), (tensor([-0.7486,  0.0477, -0.4233,  0.0477, -1.8128,  0.0477, -1.6816,  0.0477,\n",
            "        -0.6452,  0.0477,  0.3326,  0.0477,  1.1890,  0.0477,  0.2008,  0.0477,\n",
            "         0.3681,  0.0477, -1.4163,  0.0477]), tensor([  0.3238,  -7.9754, -13.2770,   3.4194,   3.6776,  -7.6515,   7.8320,\n",
            "         -6.0419,  -8.7091,  -5.6591,  -2.3184,   4.2557,  -9.2538,  -0.0218,\n",
            "          2.4791,   0.4846,  -6.8018,   1.5898,   7.6446,  -9.7691,  -5.2953,\n",
            "         -5.7672,   6.4914,  -8.0048,  -1.1121,  -0.1624,  15.9949,   6.5836,\n",
            "         -2.4063,  -3.1614,   0.3874,   0.3552])), (tensor([ 0.1025,  0.0864, -1.2935,  0.0864,  0.2120,  0.0864,  0.3998,  0.0864,\n",
            "        -1.4529,  0.0864, -0.3329,  0.0864,  1.8157,  0.0864, -1.2064,  0.0864,\n",
            "         0.7754,  0.0864, -2.1466,  0.0864]), tensor([  2.1137,   3.7323,  -4.9397,  -3.0860,   2.1480,  -4.4211,   7.7456,\n",
            "         -5.5386, -13.0120,  -4.1239,  -5.5240,   3.9308,  -5.6722,   0.1345,\n",
            "          2.7710,   5.6004,   0.1398,   3.6147,  12.6230, -10.6358,   4.7736,\n",
            "          2.4324,   5.3432,   0.8528,   0.2686,  -1.2804,  11.5360,   4.4289,\n",
            "          1.4427,   0.7885,   5.7477,  -0.0145])), (tensor([ 1.7701,  0.0813,  0.4619,  0.0813,  0.1189,  0.0813, -0.6976,  0.0813,\n",
            "         2.9156,  0.0813, -0.0063,  0.0813,  0.0895,  0.0813, -3.1208,  0.0813,\n",
            "         2.2196,  0.0813, -0.8928,  0.0813]), tensor([  3.7060,  -9.0240,  -2.8851,   3.6964,   4.7406,   4.7123,   1.8690,\n",
            "         -7.8060,  -7.3049,  -0.9429,  -6.6813,  12.3332,  -2.0004, -12.1110,\n",
            "          9.4965,   5.3677, -12.8800,   2.8275,   3.8132, -10.1810,  -5.6992,\n",
            "         -1.3725,   1.1997,   8.5027,   2.4657,   3.6687,   8.9944,   6.7644,\n",
            "         -1.3525,   1.1867,  -2.6038,   5.9928])), (tensor([ 0.7602,  0.1040,  0.4658,  0.1040, -1.0100,  0.1040, -0.3599,  0.1040,\n",
            "         0.0935,  0.1040,  1.0812,  0.1040,  0.2840,  0.1040,  0.4859,  0.1040,\n",
            "         0.6426,  0.1040,  0.0979,  0.1040]), tensor([ -5.5865,   2.4872, -16.5914,  -1.1206,   6.5751,  -4.1894,   9.5582,\n",
            "          0.6551,  -8.0002,  -8.2612,  -3.5410,   4.7391, -11.8252,  -7.2699,\n",
            "          5.7836,   1.8669,  -6.5691,   4.8365,   5.1757,  -4.6287,   5.4413,\n",
            "         -5.0277,   1.2890,  -3.5584,   1.4110,  -6.7205,  16.5129,   2.8944,\n",
            "         -4.6953,   0.1875,  -6.4037,  11.2434])), (tensor([ 0.5321,  0.0824,  1.5840,  0.0824, -0.4262,  0.0824, -0.7260,  0.0824,\n",
            "        -0.5735,  0.0824, -0.3585,  0.0824,  0.6798,  0.0824,  0.1598,  0.0824,\n",
            "         1.3587,  0.0824, -2.1061,  0.0824]), tensor([ -0.7903,  -4.9808, -13.5435,  -0.2986,  -3.1625, -11.7888,  10.8787,\n",
            "         -6.3885,  -8.2871,  -9.8603,   1.9390,   7.1494,  -5.0613,  -5.6504,\n",
            "         -0.6172,   8.4453, -10.9820,  10.8988,  -1.6049,  -4.0101,  -7.4904,\n",
            "         -1.2911,   1.3866,   1.4968,  -0.8019,  -6.8532,  14.9924,   8.1512,\n",
            "         -0.0602,  -8.0212,   0.5931,   6.3675])), (tensor([ 0.8163,  0.1050, -0.8401,  0.1050,  0.6277,  0.1050,  0.6977,  0.1050,\n",
            "         0.5295,  0.1050, -1.9574,  0.1050, -0.7916,  0.1050, -0.9738,  0.1050,\n",
            "        -0.8705,  0.1050, -0.3132,  0.1050]), tensor([  4.9960,   1.6965, -14.5156,  -2.0887,  -1.9894,  -7.4069,   8.1900,\n",
            "         -2.3971, -12.0187,  -7.3629,  -3.5541,   7.2185,  -4.5543,  -8.9769,\n",
            "          6.1595,   5.1363, -10.4975,   5.9575,   0.3151,  -3.9236,   0.0300,\n",
            "          7.0883,   0.8727,   2.0738,  -3.4366,  -1.4781,  14.5522,  11.2134,\n",
            "         -1.1715,   1.9667,  -5.7832,  11.7358])), (tensor([ 0.0849,  0.1252, -1.9537,  0.1252, -0.6024,  0.1252, -0.6192,  0.1252,\n",
            "         0.2028,  0.1252, -0.7548,  0.1252, -2.0777,  0.1252, -0.1084,  0.1252,\n",
            "         0.0283,  0.1252, -0.5437,  0.1252]), tensor([  3.8691, -10.4666, -11.0938,   4.0540,   3.8936,  -2.9858,   2.0817,\n",
            "         -5.6761,  -6.4706,  -2.4702,  -5.1330,   6.2855,  -5.3662,  -2.9960,\n",
            "          5.6006,  -1.0984,  -8.6689,  -1.7500,   3.7015,  -8.8263,  -7.4488,\n",
            "         -3.2255,   5.7777,  -5.0889,  -1.2352,   5.0083,  11.3651,   7.9000,\n",
            "         -2.9860,   1.4804,  -4.7016,   2.0369])), (tensor([ 0.7841,  0.0557,  0.0686,  0.0557,  0.5422,  0.0557,  0.2030,  0.0557,\n",
            "        -1.3422,  0.0557,  0.5828,  0.0557,  0.3281,  0.0557,  1.4685,  0.0557,\n",
            "        -0.1286,  0.0557, -1.6609,  0.0557]), tensor([-3.3900, -0.2421, -9.6959, -2.2484, -0.9122, -4.2377, 11.1755, -4.1353,\n",
            "        -6.9381, -7.9918, -0.4921, 10.6758, -7.2692, -7.9307,  1.7750, 10.8969,\n",
            "        -8.5271, 12.7191,  1.1698, -5.1479,  1.5864, -3.9561, -2.2891,  5.8611,\n",
            "         3.4383, -8.9018, 15.8619,  5.4272, -0.3257, -4.7810,  4.1172, 10.8867])), (tensor([ 0.9887,  0.1275, -0.2872,  0.1275,  1.6263,  0.1275,  0.8180,  0.1275,\n",
            "         0.9300,  0.1275, -0.1177,  0.1275, -0.7206,  0.1275, -0.1224,  0.1275,\n",
            "         0.8805,  0.1275, -0.2954,  0.1275]), tensor([ -0.5695, -13.7108, -16.5146,  -1.5132,   2.2372,  -3.9358,   2.4026,\n",
            "         -8.8829,  -5.1474,  -6.9956,  -7.4963,  15.5891,  -4.5922,  -9.3478,\n",
            "          4.7977,   8.3060, -12.5570,   6.8638,  -5.0266,  -8.7292, -10.4425,\n",
            "         -7.9863,   4.0904,   3.6548,   4.5387,  -1.5641,  14.0218,   9.0657,\n",
            "         -2.8922,  -0.3974,  -6.9663,  10.0980])), (tensor([-0.5957,  0.0983,  0.7556,  0.0983, -1.6739,  0.0983, -0.7903,  0.0983,\n",
            "        -2.6937,  0.0983, -0.3961,  0.0983, -1.2200,  0.0983, -0.0254,  0.0983,\n",
            "        -0.3351,  0.0983,  1.4519,  0.0983]), tensor([ 6.3429, -0.6938, -5.8798, -1.5529, -1.1756,  1.2857,  2.5548,  0.1178,\n",
            "        -9.6443,  2.0147, -4.6690,  1.9126, -5.3487, -1.8942,  7.2246, -3.2947,\n",
            "         0.5384, -6.5097,  7.6467, -7.5167,  5.9329,  0.2350,  3.1285, -7.1331,\n",
            "        -3.2686,  1.5661, 10.7420,  6.9536,  0.2994,  4.8035,  3.1387,  0.1889])), (tensor([-2.2720,  0.0915, -0.9333,  0.0915,  0.6390,  0.0915,  0.7999,  0.0915,\n",
            "         0.9900,  0.0915,  0.6977,  0.0915, -0.8817,  0.0915,  0.7852,  0.0915,\n",
            "         0.7802,  0.0915,  0.4764,  0.0915]), tensor([  4.4612,  -7.4861,  -7.1232,  -3.9342,   5.0980,   1.8465,  -0.4658,\n",
            "         -8.9939, -13.8921,  -0.3618, -12.8340,   9.1804,  -3.0589,  -6.8426,\n",
            "          9.8778,   3.2848,  -2.8448,  -4.1936,   9.4196, -15.5467,  -1.6285,\n",
            "         -2.7354,   9.3683,   1.8054,   2.2196,   2.5063,  10.0838,   6.4896,\n",
            "          0.4341,   4.7738,  -1.5810,  -1.1255])), (tensor([-0.1153,  0.0525, -0.7719,  0.0525, -1.2491,  0.0525, -0.3934,  0.0525,\n",
            "         2.9401,  0.0525,  0.4258,  0.0525,  0.1776,  0.0525,  0.3755,  0.0525,\n",
            "         0.3931,  0.0525,  0.5934,  0.0525]), tensor([  1.9796,   1.0600, -10.7700,  -9.6615,  -1.4546,  -3.1595,  -1.0661,\n",
            "         -2.4390,  -7.0491,  -3.9233,  -8.7526,   7.1422,   0.0580,  -4.8912,\n",
            "          3.5614,   5.7235,  -0.0379,   1.7711,  -3.4849,  -3.7027,   1.5973,\n",
            "          2.3947,   3.8593,   3.7316,   1.7748,  -2.0508,   5.1195,   5.5776,\n",
            "          0.3275,   5.9781,  -6.3772,   6.9966])), (tensor([ 0.3333,  0.0995, -0.6777,  0.0995, -1.1028,  0.0995, -0.6987,  0.0995,\n",
            "        -2.1713,  0.0995,  0.7588,  0.0995, -1.5629,  0.0995,  0.0484,  0.0995,\n",
            "         0.2601,  0.0995,  0.0119,  0.0995]), tensor([-3.0250e+00, -6.0041e+00, -1.2870e+01, -2.2798e+00,  4.0659e+00,\n",
            "         9.0824e-02,  1.4290e+00, -1.1771e+00, -3.8696e+00, -3.0147e+00,\n",
            "        -5.5769e+00,  6.2293e+00, -6.9347e+00, -5.9808e+00,  5.6292e+00,\n",
            "        -5.4909e-03, -3.7395e+00, -1.1685e+00,  1.0854e-01, -5.4856e+00,\n",
            "         7.9723e-02, -9.0384e+00,  3.0843e+00, -4.1937e+00,  2.6735e+00,\n",
            "        -3.4339e+00,  1.0665e+01,  3.0258e+00, -3.2668e+00,  1.7025e+00,\n",
            "        -5.5909e+00,  5.8200e+00])), (tensor([-0.5166,  0.0908, -0.3992,  0.0908, -0.4814,  0.0908, -0.2643,  0.0908,\n",
            "        -0.4581,  0.0908, -1.3763,  0.0908, -0.4990,  0.0908,  0.1954,  0.0908,\n",
            "        -0.4799,  0.0908, -0.3018,  0.0908]), tensor([  5.4202,  -0.1288,  -5.7885,   9.9850,   5.7520,  -3.8552,  10.4278,\n",
            "         -4.0403,  -9.6494,  -4.4398,  -0.8706,   3.8518,  -7.9452,  -2.8480,\n",
            "          5.7385,   0.3856, -12.6774,   4.5500,  12.3181,  -7.8987,  -1.5825,\n",
            "          6.1347,   1.2689,  -1.2414,  -4.1528,   6.8960,  14.0316,   7.9781,\n",
            "         -3.2293,   0.3405,  -0.6979,   5.4216])), (tensor([ 0.5717,  0.1223, -0.5939,  0.1223, -0.9081,  0.1223, -0.6230,  0.1223,\n",
            "        -0.5412,  0.1223,  0.1820,  0.1223, -1.6096,  0.1223, -0.1145,  0.1223,\n",
            "        -0.5836,  0.1223,  1.7015,  0.1223]), tensor([ 3.6325e+00, -1.9824e+00, -5.7451e+00, -2.7780e+00, -1.3984e+00,\n",
            "         2.8412e+00,  5.5224e+00, -3.3881e+00, -6.7370e+00,  1.0142e-02,\n",
            "        -6.4601e+00,  1.1429e+01, -8.4502e+00,  4.0342e-01,  4.1428e+00,\n",
            "         5.6362e+00,  2.3103e-02,  3.1524e+00,  7.6716e+00, -1.1254e+01,\n",
            "         7.7832e+00, -5.3619e+00,  8.5349e-01, -1.5997e+00,  3.1749e+00,\n",
            "        -2.0294e-01,  1.6124e+01,  7.8837e+00,  3.9544e-01,  4.6229e+00,\n",
            "         1.0998e+01,  5.7675e+00])), (tensor([-0.8155,  0.0884, -0.6057,  0.0884, -0.1901,  0.0884,  1.1099,  0.0884,\n",
            "         1.2277,  0.0884, -0.6361,  0.0884, -0.3691,  0.0884, -0.1099,  0.0884,\n",
            "         0.6552,  0.0884, -0.0715,  0.0884]), tensor([ 2.6305e+00, -1.4104e-01, -1.4048e+01, -9.4529e+00, -4.5309e+00,\n",
            "        -4.0779e+00,  5.8769e+00, -4.7649e+00, -1.0196e+01, -6.4674e+00,\n",
            "        -7.9447e+00,  1.4403e+01, -4.4774e+00, -7.3921e+00,  4.2450e+00,\n",
            "         1.1854e+01, -4.6830e+00,  8.9515e+00, -2.1319e+00, -7.3774e+00,\n",
            "         3.4440e+00,  1.2665e-02,  1.1083e+00,  5.9276e+00,  3.0598e+00,\n",
            "        -5.5494e+00,  1.5732e+01,  1.0700e+01,  8.9757e-01,  3.6155e+00,\n",
            "         1.0360e+00,  1.2901e+01])), (tensor([-2.7708,  0.0910,  1.3596,  0.0910,  1.1938,  0.0910, -0.7438,  0.0910,\n",
            "         1.2383,  0.0910,  0.1173,  0.0910,  1.2155,  0.0910, -0.9260,  0.0910,\n",
            "        -0.6305,  0.0910,  0.1102,  0.0910]), tensor([  6.5684,  -1.8721,  -7.0438,   1.8448,  -0.6122,  -3.6450,  11.3081,\n",
            "         -8.3179, -13.9005,  -5.3705,  -4.2368,  12.3429,  -6.3085,  -6.8523,\n",
            "          6.0692,   9.7347, -11.7015,   9.4524,   9.0141, -11.9091,  -0.3714,\n",
            "          4.1970,   1.0710,   6.0092,  -0.6064,   0.9886,  18.4194,  12.0185,\n",
            "          0.6447,  -0.3671,   6.1264,   8.0790])), (tensor([-1.2624,  0.1045,  0.4269,  0.1045, -2.2169,  0.1045, -1.0168,  0.1045,\n",
            "        -1.3375,  0.1045, -1.1839,  0.1045, -0.2288,  0.1045,  0.6673,  0.1045,\n",
            "        -1.0245,  0.1045,  0.5220,  0.1045]), tensor([  1.4486,   4.8306,  -8.6830,   1.9907,   6.2206,  -6.2937,  10.9721,\n",
            "         -5.6506,  -9.0576,  -7.2802,  -6.0519,   7.6600, -10.6672,   5.6445,\n",
            "          0.3414,   7.0949,  -3.4148,  10.0727,  14.4511, -11.1968,   5.0585,\n",
            "          2.5472,   4.1775,  -0.5084,   1.7763,   3.9686,  16.1329,   6.0655,\n",
            "         -2.4000,   3.7792,   4.5881,   6.8099])), (tensor([ 0.2471,  0.0581, -0.4055,  0.0581, -0.9091,  0.0581, -0.6467,  0.0581,\n",
            "        -0.2849,  0.0581,  0.1394,  0.0581, -1.6959,  0.0581,  0.1827,  0.0581,\n",
            "        -1.7794,  0.0581,  0.6686,  0.0581]), tensor([  2.3875,   3.5610,  -9.6246,  -6.0417,  -0.4618, -10.3775,   8.2405,\n",
            "         -6.7964, -17.9864,  -8.2039,  -4.2914,   2.4322,  -1.1580,  -8.4307,\n",
            "          4.3690,   7.3647,  -5.2871,   4.6807,   6.2372,  -7.2164,  -1.5148,\n",
            "          7.9754,   6.2488,   5.4294,  -2.7361,  -5.6522,   9.8179,   6.2746,\n",
            "          2.5393,  -3.0992,  -2.8477,   1.2976])), (tensor([-0.0762,  0.1109, -0.5747,  0.1109, -0.4215,  0.1109,  1.8006,  0.1109,\n",
            "        -0.0374,  0.1109,  1.7642,  0.1109,  0.7196,  0.1109,  0.8383,  0.1109,\n",
            "         0.0793,  0.1109,  0.0140,  0.1109]), tensor([  1.9615,  -2.9671, -14.3697,  -2.7455,   6.6794,  -1.1806,   5.6690,\n",
            "         -6.3426, -13.2810,  -4.8004, -12.0437,  11.3768,  -9.9363,  -5.0242,\n",
            "          8.9327,   4.6855,  -5.0874,   1.8265,  10.6280, -14.6626,   3.4283,\n",
            "         -3.5622,   6.9776,  -1.1367,   2.7472,   0.9817,  18.1712,   8.1920,\n",
            "         -2.6132,   6.0826,  -2.1212,   7.4472])), (tensor([ 0.4413,  0.1157,  1.2739,  0.1157, -0.8662,  0.1157, -0.3641,  0.1157,\n",
            "        -0.7297,  0.1157, -1.6808,  0.1157, -1.3104,  0.1157, -1.2666,  0.1157,\n",
            "        -1.5399,  0.1157, -0.3429,  0.1157]), tensor([  1.9562,   0.1433,  -8.7177,   1.8101,   5.5551,  -3.6532,   7.7298,\n",
            "         -8.1948,  -8.0327,  -8.7389,  -7.1892,  14.4746,  -4.3315,  -7.6274,\n",
            "          4.4331,  12.2974, -14.0004,  14.2890,   3.8499,  -8.2442,  -3.0593,\n",
            "          5.4848,   0.6616,  12.3667,   3.4124,   3.3600,  12.2836,   8.1580,\n",
            "         -2.6992,   2.8135,  -4.9661,  14.2896])), (tensor([ 1.4354,  0.0609, -1.6708,  0.0609, -0.9836,  0.0609,  1.2450,  0.0609,\n",
            "        -0.3683,  0.0609, -0.3175,  0.0609,  0.3605,  0.0609,  1.9843,  0.0609,\n",
            "        -0.4464,  0.0609, -1.3883,  0.0609]), tensor([  0.0502,   2.5985,  -4.0069,  -2.3377,  -1.1985,  -3.1255,  10.5034,\n",
            "         -4.2356, -11.6677,  -4.7413,  -0.7106,   5.4688,  -5.3491,  -6.5270,\n",
            "          3.3713,   7.6924,  -4.4956,   7.0233,   7.8159,  -7.2632,   4.0437,\n",
            "          0.3740,   0.0478,   4.4213,   0.4199,  -7.0483,  13.0569,   4.5265,\n",
            "          2.0763,  -4.3877,   7.5249,   3.4473])), (tensor([-1.2451,  0.1026, -0.3015,  0.1026,  1.1474,  0.1026, -0.6574,  0.1026,\n",
            "        -0.5490,  0.1026, -0.3163,  0.1026,  0.4368,  0.1026, -0.4262,  0.1026,\n",
            "         0.2168,  0.1026,  0.6076,  0.1026]), tensor([  3.6186,  -4.4273,  -6.0599,  -2.1499,   1.9078,  -0.6456,   4.6237,\n",
            "         -6.9889, -15.5170,  -2.2519,  -6.2988,   6.1051,  -2.7046, -11.6611,\n",
            "          9.6931,   3.6839,  -7.0132,  -1.2367,   7.9320, -11.2249,  -1.6925,\n",
            "          0.4166,   5.1783,   3.6189,  -0.8566,  -2.5085,  11.1558,   6.5185,\n",
            "          1.4221,  -1.2105,  -0.4561,   0.1821])), (tensor([-0.7601,  0.0856, -0.3631,  0.0856,  0.0984,  0.0856,  2.2234,  0.0856,\n",
            "        -0.0165,  0.0856, -0.5872,  0.0856,  0.9508,  0.0856, -0.3827,  0.0856,\n",
            "        -0.4519,  0.0856, -0.6886,  0.0856]), tensor([  5.0514,  -5.3584, -14.4378,   4.5343,  -2.0087,  -1.3098,  10.2219,\n",
            "         -1.1047,  -3.8622,  -3.4099,  -2.0756,  13.3495, -13.2691,  -0.5708,\n",
            "          4.4287,   3.1172,  -9.8918,   6.7607,   3.2596,  -7.8452,   3.5719,\n",
            "         -5.0930,  -2.6657,  -7.0496,  -0.2763,   2.3940,  23.7974,  13.5882,\n",
            "         -3.8625,   3.8023,   6.1471,  14.3446])), (tensor([ 0.2294,  0.1098,  1.4112,  0.1098, -1.6916,  0.1098, -0.8997,  0.1098,\n",
            "        -0.4657,  0.1098, -0.8099,  0.1098, -1.8636,  0.1098,  0.6629,  0.1098,\n",
            "         0.0744,  0.1098,  0.1686,  0.1098]), tensor([  0.8252,  -2.8245, -14.8533,   4.1461,  -1.6922,  -6.3577,  12.0625,\n",
            "          0.7955,  -8.6572,  -6.4688,   4.0328,   3.3916,  -9.3680, -10.2669,\n",
            "          6.0824,  -0.4075, -13.3958,   3.8517,   0.7695,  -1.6270,  -1.0198,\n",
            "         -1.3512,  -2.4167,  -5.5949,  -4.9433,  -5.8678,  18.5035,   8.9671,\n",
            "         -2.7993,  -5.2214,  -2.4224,   9.7135])), (tensor([-0.1624,  0.1100, -1.6879,  0.1100,  1.9725,  0.1100, -0.3255,  0.1100,\n",
            "        -0.5773,  0.1100,  0.5081,  0.1100, -1.5774,  0.1100,  0.0428,  0.1100,\n",
            "         0.0410,  0.1100, -0.7495,  0.1100]), tensor([ -1.3578,  -8.8000,  -9.3746,   0.1273,   4.3565,  -8.4148,   4.8659,\n",
            "        -10.7326,  -8.1984,  -6.7365,  -4.7231,   6.3729,  -4.2572,   0.2737,\n",
            "         -0.4022,   6.2424,  -5.2177,   5.0591,   5.9050, -10.9140,  -9.5119,\n",
            "         -4.9502,   8.6860,   0.0469,   2.4733,  -0.6016,  10.0160,   3.9677,\n",
            "         -0.4021,  -4.4566,   0.2284,  -2.4299])), (tensor([ 0.4452,  0.0805, -1.2517,  0.0805, -0.1934,  0.0805,  0.3726,  0.0805,\n",
            "        -0.7575,  0.0805,  0.8606,  0.0805, -1.4408,  0.0805, -0.7137,  0.0805,\n",
            "         0.8899,  0.0805,  0.1649,  0.0805]), tensor([  5.4857,  -3.7584, -14.2566,  -0.5809,  -3.3032,  -7.3768,   4.1340,\n",
            "         -3.3413,  -3.1843,  -5.8171,  -3.8879,  10.2077,  -4.3033,   0.6011,\n",
            "          0.3182,   5.3771,  -7.6140,   7.2181,  -4.1573,  -3.4366,  -3.7011,\n",
            "          2.5857,   1.2202,  -1.3587,  -0.8169,   3.7806,  12.5052,  11.7406,\n",
            "         -2.2104,   4.4003,  -2.9603,  11.2305]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the router with input shape being len(flattened_context) and output shape being encode_dim"
      ],
      "metadata": {
        "id": "qG-rdqh8UzVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# preparing the training data\n",
        "contexts = torch.stack([pair[0] for pair in training_data]) # shape = [100, len(flattened_context)]\n",
        "targets = torch.stack([pair[1] for pair in training_data]) # shape = [100, encode_dim]\n",
        "\n",
        "dataset = TensorDataset(contexts, targets)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)  # 16 samples per batch\n",
        "\n",
        "\n",
        "# defining the router\n",
        "in_dim = len(training_data[0][0]) # length of flattened context\n",
        "w = 28\n",
        "out_dim = EMBED_DIM # gets decoded into mask\n",
        "\n",
        "d = 10 # depth\n",
        "\n",
        "layer_sizes = [[in_dim, w]]\n",
        "for _ in range(d):\n",
        "    layer_sizes.append([w, w])\n",
        "layer_sizes.append([w, out_dim])\n",
        "\n",
        "Router = ClassicNetwork(layer_sizes)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(Router.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "thXd7z5RU-FC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train it, minimizing Router(context) - AE(mask that generated the context)  ... such that router gets better at producing embedded masks that might have generated this context"
      ],
      "metadata": {
        "id": "et7GHtqhWF65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_context, batch_target in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        pred = Router(batch_context)  # shape [batch_size, encode_dim], all predicted y for batch\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(pred, batch_target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "a5dwr3I6WJIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53087146-f107-4878-afde-c2f33eaad3fc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 16.4615\n",
            "Epoch 2/500, Loss: 15.8240\n",
            "Epoch 3/500, Loss: 16.2293\n",
            "Epoch 4/500, Loss: 15.9058\n",
            "Epoch 5/500, Loss: 15.7197\n",
            "Epoch 6/500, Loss: 15.8705\n",
            "Epoch 7/500, Loss: 15.8098\n",
            "Epoch 8/500, Loss: 16.0492\n",
            "Epoch 9/500, Loss: 15.9528\n",
            "Epoch 10/500, Loss: 16.2993\n",
            "Epoch 11/500, Loss: 15.4374\n",
            "Epoch 12/500, Loss: 16.1634\n",
            "Epoch 13/500, Loss: 16.0046\n",
            "Epoch 14/500, Loss: 15.9766\n",
            "Epoch 15/500, Loss: 15.7543\n",
            "Epoch 16/500, Loss: 16.2283\n",
            "Epoch 17/500, Loss: 15.8279\n",
            "Epoch 18/500, Loss: 15.6752\n",
            "Epoch 19/500, Loss: 15.2433\n",
            "Epoch 20/500, Loss: 15.8695\n",
            "Epoch 21/500, Loss: 16.0852\n",
            "Epoch 22/500, Loss: 15.9280\n",
            "Epoch 23/500, Loss: 16.2254\n",
            "Epoch 24/500, Loss: 15.7834\n",
            "Epoch 25/500, Loss: 15.7102\n",
            "Epoch 26/500, Loss: 15.7469\n",
            "Epoch 27/500, Loss: 16.1146\n",
            "Epoch 28/500, Loss: 15.6475\n",
            "Epoch 29/500, Loss: 16.0112\n",
            "Epoch 30/500, Loss: 16.1642\n",
            "Epoch 31/500, Loss: 16.1263\n",
            "Epoch 32/500, Loss: 15.2042\n",
            "Epoch 33/500, Loss: 15.8985\n",
            "Epoch 34/500, Loss: 15.6214\n",
            "Epoch 35/500, Loss: 15.9498\n",
            "Epoch 36/500, Loss: 15.8693\n",
            "Epoch 37/500, Loss: 15.9604\n",
            "Epoch 38/500, Loss: 15.5709\n",
            "Epoch 39/500, Loss: 15.4669\n",
            "Epoch 40/500, Loss: 15.8753\n",
            "Epoch 41/500, Loss: 15.7730\n",
            "Epoch 42/500, Loss: 15.5909\n",
            "Epoch 43/500, Loss: 15.5373\n",
            "Epoch 44/500, Loss: 15.2830\n",
            "Epoch 45/500, Loss: 15.5834\n",
            "Epoch 46/500, Loss: 15.9253\n",
            "Epoch 47/500, Loss: 15.3120\n",
            "Epoch 48/500, Loss: 15.9446\n",
            "Epoch 49/500, Loss: 15.4513\n",
            "Epoch 50/500, Loss: 15.7382\n",
            "Epoch 51/500, Loss: 15.3650\n",
            "Epoch 52/500, Loss: 15.5852\n",
            "Epoch 53/500, Loss: 15.7894\n",
            "Epoch 54/500, Loss: 15.4102\n",
            "Epoch 55/500, Loss: 15.6975\n",
            "Epoch 56/500, Loss: 15.7763\n",
            "Epoch 57/500, Loss: 15.5972\n",
            "Epoch 58/500, Loss: 15.7534\n",
            "Epoch 59/500, Loss: 15.8289\n",
            "Epoch 60/500, Loss: 15.7575\n",
            "Epoch 61/500, Loss: 15.4022\n",
            "Epoch 62/500, Loss: 15.5078\n",
            "Epoch 63/500, Loss: 15.4895\n",
            "Epoch 64/500, Loss: 15.5444\n",
            "Epoch 65/500, Loss: 15.3248\n",
            "Epoch 66/500, Loss: 15.4763\n",
            "Epoch 67/500, Loss: 15.5258\n",
            "Epoch 68/500, Loss: 15.4652\n",
            "Epoch 69/500, Loss: 15.4530\n",
            "Epoch 70/500, Loss: 15.1987\n",
            "Epoch 71/500, Loss: 15.3223\n",
            "Epoch 72/500, Loss: 15.2344\n",
            "Epoch 73/500, Loss: 15.8399\n",
            "Epoch 74/500, Loss: 15.7437\n",
            "Epoch 75/500, Loss: 15.4322\n",
            "Epoch 76/500, Loss: 15.6970\n",
            "Epoch 77/500, Loss: 15.0352\n",
            "Epoch 78/500, Loss: 15.2602\n",
            "Epoch 79/500, Loss: 15.4573\n",
            "Epoch 80/500, Loss: 15.2187\n",
            "Epoch 81/500, Loss: 15.3496\n",
            "Epoch 82/500, Loss: 15.4575\n",
            "Epoch 83/500, Loss: 15.5023\n",
            "Epoch 84/500, Loss: 15.0492\n",
            "Epoch 85/500, Loss: 15.2434\n",
            "Epoch 86/500, Loss: 15.6003\n",
            "Epoch 87/500, Loss: 14.9497\n",
            "Epoch 88/500, Loss: 15.4810\n",
            "Epoch 89/500, Loss: 14.8851\n",
            "Epoch 90/500, Loss: 15.1464\n",
            "Epoch 91/500, Loss: 15.3160\n",
            "Epoch 92/500, Loss: 14.8691\n",
            "Epoch 93/500, Loss: 15.3166\n",
            "Epoch 94/500, Loss: 15.0730\n",
            "Epoch 95/500, Loss: 15.0077\n",
            "Epoch 96/500, Loss: 14.7352\n",
            "Epoch 97/500, Loss: 15.3164\n",
            "Epoch 98/500, Loss: 15.2811\n",
            "Epoch 99/500, Loss: 15.2389\n",
            "Epoch 100/500, Loss: 14.8742\n",
            "Epoch 101/500, Loss: 15.0454\n",
            "Epoch 102/500, Loss: 15.1015\n",
            "Epoch 103/500, Loss: 14.9191\n",
            "Epoch 104/500, Loss: 15.1823\n",
            "Epoch 105/500, Loss: 15.3713\n",
            "Epoch 106/500, Loss: 15.2359\n",
            "Epoch 107/500, Loss: 14.9623\n",
            "Epoch 108/500, Loss: 15.3153\n",
            "Epoch 109/500, Loss: 14.9981\n",
            "Epoch 110/500, Loss: 14.9374\n",
            "Epoch 111/500, Loss: 15.2463\n",
            "Epoch 112/500, Loss: 15.4711\n",
            "Epoch 113/500, Loss: 15.0455\n",
            "Epoch 114/500, Loss: 15.3487\n",
            "Epoch 115/500, Loss: 15.0225\n",
            "Epoch 116/500, Loss: 14.8663\n",
            "Epoch 117/500, Loss: 15.2735\n",
            "Epoch 118/500, Loss: 15.0627\n",
            "Epoch 119/500, Loss: 14.9833\n",
            "Epoch 120/500, Loss: 15.1046\n",
            "Epoch 121/500, Loss: 14.9636\n",
            "Epoch 122/500, Loss: 15.2167\n",
            "Epoch 123/500, Loss: 14.9974\n",
            "Epoch 124/500, Loss: 15.1157\n",
            "Epoch 125/500, Loss: 15.2500\n",
            "Epoch 126/500, Loss: 14.7680\n",
            "Epoch 127/500, Loss: 15.1531\n",
            "Epoch 128/500, Loss: 15.3285\n",
            "Epoch 129/500, Loss: 14.5874\n",
            "Epoch 130/500, Loss: 15.0363\n",
            "Epoch 131/500, Loss: 14.5672\n",
            "Epoch 132/500, Loss: 15.0046\n",
            "Epoch 133/500, Loss: 15.1771\n",
            "Epoch 134/500, Loss: 14.7228\n",
            "Epoch 135/500, Loss: 15.0457\n",
            "Epoch 136/500, Loss: 15.1665\n",
            "Epoch 137/500, Loss: 15.2957\n",
            "Epoch 138/500, Loss: 15.0604\n",
            "Epoch 139/500, Loss: 14.6241\n",
            "Epoch 140/500, Loss: 14.7539\n",
            "Epoch 141/500, Loss: 14.9912\n",
            "Epoch 142/500, Loss: 14.7925\n",
            "Epoch 143/500, Loss: 14.8618\n",
            "Epoch 144/500, Loss: 14.9071\n",
            "Epoch 145/500, Loss: 15.1050\n",
            "Epoch 146/500, Loss: 15.0896\n",
            "Epoch 147/500, Loss: 15.0038\n",
            "Epoch 148/500, Loss: 14.6210\n",
            "Epoch 149/500, Loss: 14.8690\n",
            "Epoch 150/500, Loss: 15.1843\n",
            "Epoch 151/500, Loss: 14.9537\n",
            "Epoch 152/500, Loss: 15.0041\n",
            "Epoch 153/500, Loss: 15.1181\n",
            "Epoch 154/500, Loss: 15.0842\n",
            "Epoch 155/500, Loss: 14.8887\n",
            "Epoch 156/500, Loss: 14.8048\n",
            "Epoch 157/500, Loss: 14.8097\n",
            "Epoch 158/500, Loss: 15.0178\n",
            "Epoch 159/500, Loss: 14.7046\n",
            "Epoch 160/500, Loss: 15.1305\n",
            "Epoch 161/500, Loss: 14.7297\n",
            "Epoch 162/500, Loss: 15.0056\n",
            "Epoch 163/500, Loss: 14.9679\n",
            "Epoch 164/500, Loss: 15.0110\n",
            "Epoch 165/500, Loss: 15.1019\n",
            "Epoch 166/500, Loss: 15.0653\n",
            "Epoch 167/500, Loss: 14.6508\n",
            "Epoch 168/500, Loss: 14.5370\n",
            "Epoch 169/500, Loss: 15.0214\n",
            "Epoch 170/500, Loss: 14.9872\n",
            "Epoch 171/500, Loss: 14.9787\n",
            "Epoch 172/500, Loss: 14.6914\n",
            "Epoch 173/500, Loss: 14.9509\n",
            "Epoch 174/500, Loss: 15.2682\n",
            "Epoch 175/500, Loss: 14.9334\n",
            "Epoch 176/500, Loss: 15.1816\n",
            "Epoch 177/500, Loss: 15.1304\n",
            "Epoch 178/500, Loss: 14.9673\n",
            "Epoch 179/500, Loss: 14.8938\n",
            "Epoch 180/500, Loss: 14.9361\n",
            "Epoch 181/500, Loss: 14.9223\n",
            "Epoch 182/500, Loss: 14.9049\n",
            "Epoch 183/500, Loss: 14.5863\n",
            "Epoch 184/500, Loss: 14.9550\n",
            "Epoch 185/500, Loss: 15.3579\n",
            "Epoch 186/500, Loss: 14.7635\n",
            "Epoch 187/500, Loss: 15.3277\n",
            "Epoch 188/500, Loss: 15.2079\n",
            "Epoch 189/500, Loss: 15.0602\n",
            "Epoch 190/500, Loss: 14.9647\n",
            "Epoch 191/500, Loss: 15.0457\n",
            "Epoch 192/500, Loss: 14.7884\n",
            "Epoch 193/500, Loss: 14.8175\n",
            "Epoch 194/500, Loss: 14.6807\n",
            "Epoch 195/500, Loss: 14.7794\n",
            "Epoch 196/500, Loss: 14.9542\n",
            "Epoch 197/500, Loss: 14.7896\n",
            "Epoch 198/500, Loss: 15.3123\n",
            "Epoch 199/500, Loss: 15.1028\n",
            "Epoch 200/500, Loss: 15.1587\n",
            "Epoch 201/500, Loss: 14.8806\n",
            "Epoch 202/500, Loss: 14.7850\n",
            "Epoch 203/500, Loss: 14.6362\n",
            "Epoch 204/500, Loss: 14.9783\n",
            "Epoch 205/500, Loss: 15.1302\n",
            "Epoch 206/500, Loss: 14.8210\n",
            "Epoch 207/500, Loss: 14.7666\n",
            "Epoch 208/500, Loss: 15.2337\n",
            "Epoch 209/500, Loss: 14.8568\n",
            "Epoch 210/500, Loss: 14.9834\n",
            "Epoch 211/500, Loss: 15.0944\n",
            "Epoch 212/500, Loss: 15.1170\n",
            "Epoch 213/500, Loss: 15.0385\n",
            "Epoch 214/500, Loss: 14.9931\n",
            "Epoch 215/500, Loss: 14.7092\n",
            "Epoch 216/500, Loss: 14.6564\n",
            "Epoch 217/500, Loss: 15.1585\n",
            "Epoch 218/500, Loss: 15.2691\n",
            "Epoch 219/500, Loss: 15.0016\n",
            "Epoch 220/500, Loss: 14.8280\n",
            "Epoch 221/500, Loss: 14.7155\n",
            "Epoch 222/500, Loss: 14.5467\n",
            "Epoch 223/500, Loss: 14.9090\n",
            "Epoch 224/500, Loss: 15.2756\n",
            "Epoch 225/500, Loss: 14.9235\n",
            "Epoch 226/500, Loss: 14.8091\n",
            "Epoch 227/500, Loss: 15.1379\n",
            "Epoch 228/500, Loss: 14.7047\n",
            "Epoch 229/500, Loss: 14.6865\n",
            "Epoch 230/500, Loss: 14.6246\n",
            "Epoch 231/500, Loss: 15.0336\n",
            "Epoch 232/500, Loss: 15.2368\n",
            "Epoch 233/500, Loss: 14.3742\n",
            "Epoch 234/500, Loss: 14.8675\n",
            "Epoch 235/500, Loss: 14.9330\n",
            "Epoch 236/500, Loss: 14.7004\n",
            "Epoch 237/500, Loss: 14.7283\n",
            "Epoch 238/500, Loss: 14.8632\n",
            "Epoch 239/500, Loss: 15.1307\n",
            "Epoch 240/500, Loss: 14.9069\n",
            "Epoch 241/500, Loss: 14.9536\n",
            "Epoch 242/500, Loss: 14.7482\n",
            "Epoch 243/500, Loss: 14.7916\n",
            "Epoch 244/500, Loss: 15.0737\n",
            "Epoch 245/500, Loss: 15.2006\n",
            "Epoch 246/500, Loss: 15.0680\n",
            "Epoch 247/500, Loss: 15.2224\n",
            "Epoch 248/500, Loss: 14.3531\n",
            "Epoch 249/500, Loss: 14.7604\n",
            "Epoch 250/500, Loss: 14.8562\n",
            "Epoch 251/500, Loss: 14.6673\n",
            "Epoch 252/500, Loss: 14.8495\n",
            "Epoch 253/500, Loss: 14.8167\n",
            "Epoch 254/500, Loss: 15.1291\n",
            "Epoch 255/500, Loss: 15.0261\n",
            "Epoch 256/500, Loss: 14.9254\n",
            "Epoch 257/500, Loss: 14.6537\n",
            "Epoch 258/500, Loss: 15.0203\n",
            "Epoch 259/500, Loss: 14.4571\n",
            "Epoch 260/500, Loss: 14.8662\n",
            "Epoch 261/500, Loss: 14.9669\n",
            "Epoch 262/500, Loss: 14.5165\n",
            "Epoch 263/500, Loss: 14.9947\n",
            "Epoch 264/500, Loss: 14.9947\n",
            "Epoch 265/500, Loss: 15.2534\n",
            "Epoch 266/500, Loss: 15.0827\n",
            "Epoch 267/500, Loss: 14.7795\n",
            "Epoch 268/500, Loss: 15.1782\n",
            "Epoch 269/500, Loss: 14.8111\n",
            "Epoch 270/500, Loss: 14.9816\n",
            "Epoch 271/500, Loss: 15.0376\n",
            "Epoch 272/500, Loss: 15.3808\n",
            "Epoch 273/500, Loss: 14.7598\n",
            "Epoch 274/500, Loss: 14.9560\n",
            "Epoch 275/500, Loss: 15.0445\n",
            "Epoch 276/500, Loss: 15.0603\n",
            "Epoch 277/500, Loss: 14.9272\n",
            "Epoch 278/500, Loss: 15.3200\n",
            "Epoch 279/500, Loss: 15.0512\n",
            "Epoch 280/500, Loss: 14.7124\n",
            "Epoch 281/500, Loss: 15.0036\n",
            "Epoch 282/500, Loss: 14.9384\n",
            "Epoch 283/500, Loss: 15.0683\n",
            "Epoch 284/500, Loss: 15.0688\n",
            "Epoch 285/500, Loss: 15.0876\n",
            "Epoch 286/500, Loss: 14.6988\n",
            "Epoch 287/500, Loss: 14.8359\n",
            "Epoch 288/500, Loss: 15.2133\n",
            "Epoch 289/500, Loss: 14.8999\n",
            "Epoch 290/500, Loss: 14.9737\n",
            "Epoch 291/500, Loss: 14.8220\n",
            "Epoch 292/500, Loss: 15.2145\n",
            "Epoch 293/500, Loss: 14.4956\n",
            "Epoch 294/500, Loss: 14.9581\n",
            "Epoch 295/500, Loss: 14.4190\n",
            "Epoch 296/500, Loss: 14.7075\n",
            "Epoch 297/500, Loss: 14.7515\n",
            "Epoch 298/500, Loss: 14.7616\n",
            "Epoch 299/500, Loss: 15.1347\n",
            "Epoch 300/500, Loss: 14.6211\n",
            "Epoch 301/500, Loss: 14.4108\n",
            "Epoch 302/500, Loss: 15.1933\n",
            "Epoch 303/500, Loss: 15.0297\n",
            "Epoch 304/500, Loss: 14.8153\n",
            "Epoch 305/500, Loss: 15.0548\n",
            "Epoch 306/500, Loss: 15.2031\n",
            "Epoch 307/500, Loss: 15.0872\n",
            "Epoch 308/500, Loss: 14.9654\n",
            "Epoch 309/500, Loss: 14.7199\n",
            "Epoch 310/500, Loss: 14.8434\n",
            "Epoch 311/500, Loss: 14.6863\n",
            "Epoch 312/500, Loss: 15.0384\n",
            "Epoch 313/500, Loss: 14.8448\n",
            "Epoch 314/500, Loss: 15.3012\n",
            "Epoch 315/500, Loss: 15.2840\n",
            "Epoch 316/500, Loss: 14.6387\n",
            "Epoch 317/500, Loss: 15.0063\n",
            "Epoch 318/500, Loss: 15.0257\n",
            "Epoch 319/500, Loss: 14.9122\n",
            "Epoch 320/500, Loss: 15.0143\n",
            "Epoch 321/500, Loss: 14.2629\n",
            "Epoch 322/500, Loss: 14.9296\n",
            "Epoch 323/500, Loss: 14.8058\n",
            "Epoch 324/500, Loss: 15.2036\n",
            "Epoch 325/500, Loss: 14.5437\n",
            "Epoch 326/500, Loss: 14.8164\n",
            "Epoch 327/500, Loss: 14.7761\n",
            "Epoch 328/500, Loss: 14.8447\n",
            "Epoch 329/500, Loss: 14.3915\n",
            "Epoch 330/500, Loss: 14.6772\n",
            "Epoch 331/500, Loss: 14.8835\n",
            "Epoch 332/500, Loss: 14.7251\n",
            "Epoch 333/500, Loss: 15.1378\n",
            "Epoch 334/500, Loss: 15.1346\n",
            "Epoch 335/500, Loss: 14.8341\n",
            "Epoch 336/500, Loss: 14.7653\n",
            "Epoch 337/500, Loss: 14.7727\n",
            "Epoch 338/500, Loss: 14.9583\n",
            "Epoch 339/500, Loss: 14.8245\n",
            "Epoch 340/500, Loss: 14.4254\n",
            "Epoch 341/500, Loss: 15.1500\n",
            "Epoch 342/500, Loss: 14.9893\n",
            "Epoch 343/500, Loss: 14.9978\n",
            "Epoch 344/500, Loss: 14.5412\n",
            "Epoch 345/500, Loss: 14.8740\n",
            "Epoch 346/500, Loss: 14.8232\n",
            "Epoch 347/500, Loss: 14.6947\n",
            "Epoch 348/500, Loss: 15.4685\n",
            "Epoch 349/500, Loss: 14.8754\n",
            "Epoch 350/500, Loss: 14.7344\n",
            "Epoch 351/500, Loss: 14.9168\n",
            "Epoch 352/500, Loss: 14.7423\n",
            "Epoch 353/500, Loss: 14.8754\n",
            "Epoch 354/500, Loss: 14.6581\n",
            "Epoch 355/500, Loss: 14.6842\n",
            "Epoch 356/500, Loss: 15.0070\n",
            "Epoch 357/500, Loss: 14.5605\n",
            "Epoch 358/500, Loss: 14.9106\n",
            "Epoch 359/500, Loss: 14.9380\n",
            "Epoch 360/500, Loss: 14.7110\n",
            "Epoch 361/500, Loss: 15.2643\n",
            "Epoch 362/500, Loss: 15.0841\n",
            "Epoch 363/500, Loss: 14.8272\n",
            "Epoch 364/500, Loss: 15.4676\n",
            "Epoch 365/500, Loss: 14.8965\n",
            "Epoch 366/500, Loss: 14.9590\n",
            "Epoch 367/500, Loss: 14.7369\n",
            "Epoch 368/500, Loss: 14.9455\n",
            "Epoch 369/500, Loss: 14.8216\n",
            "Epoch 370/500, Loss: 14.4076\n",
            "Epoch 371/500, Loss: 14.6301\n",
            "Epoch 372/500, Loss: 14.8479\n",
            "Epoch 373/500, Loss: 14.9064\n",
            "Epoch 374/500, Loss: 14.7210\n",
            "Epoch 375/500, Loss: 15.0221\n",
            "Epoch 376/500, Loss: 14.8206\n",
            "Epoch 377/500, Loss: 14.8137\n",
            "Epoch 378/500, Loss: 14.5681\n",
            "Epoch 379/500, Loss: 14.7925\n",
            "Epoch 380/500, Loss: 15.0084\n",
            "Epoch 381/500, Loss: 15.2214\n",
            "Epoch 382/500, Loss: 14.7576\n",
            "Epoch 383/500, Loss: 14.5675\n",
            "Epoch 384/500, Loss: 14.6664\n",
            "Epoch 385/500, Loss: 15.1595\n",
            "Epoch 386/500, Loss: 14.7948\n",
            "Epoch 387/500, Loss: 15.1502\n",
            "Epoch 388/500, Loss: 14.8729\n",
            "Epoch 389/500, Loss: 15.1169\n",
            "Epoch 390/500, Loss: 15.3147\n",
            "Epoch 391/500, Loss: 15.0076\n",
            "Epoch 392/500, Loss: 14.6985\n",
            "Epoch 393/500, Loss: 14.5523\n",
            "Epoch 394/500, Loss: 15.0959\n",
            "Epoch 395/500, Loss: 15.0558\n",
            "Epoch 396/500, Loss: 14.6761\n",
            "Epoch 397/500, Loss: 15.2062\n",
            "Epoch 398/500, Loss: 15.4243\n",
            "Epoch 399/500, Loss: 14.6347\n",
            "Epoch 400/500, Loss: 14.8428\n",
            "Epoch 401/500, Loss: 15.2038\n",
            "Epoch 402/500, Loss: 15.1585\n",
            "Epoch 403/500, Loss: 15.1872\n",
            "Epoch 404/500, Loss: 14.7834\n",
            "Epoch 405/500, Loss: 14.7871\n",
            "Epoch 406/500, Loss: 14.7833\n",
            "Epoch 407/500, Loss: 15.0584\n",
            "Epoch 408/500, Loss: 14.9973\n",
            "Epoch 409/500, Loss: 14.7788\n",
            "Epoch 410/500, Loss: 14.7095\n",
            "Epoch 411/500, Loss: 14.9176\n",
            "Epoch 412/500, Loss: 15.1091\n",
            "Epoch 413/500, Loss: 14.5131\n",
            "Epoch 414/500, Loss: 15.1053\n",
            "Epoch 415/500, Loss: 15.1183\n",
            "Epoch 416/500, Loss: 14.5364\n",
            "Epoch 417/500, Loss: 14.9894\n",
            "Epoch 418/500, Loss: 14.9014\n",
            "Epoch 419/500, Loss: 14.8174\n",
            "Epoch 420/500, Loss: 14.4921\n",
            "Epoch 421/500, Loss: 14.7812\n",
            "Epoch 422/500, Loss: 14.5397\n",
            "Epoch 423/500, Loss: 14.7493\n",
            "Epoch 424/500, Loss: 14.6151\n",
            "Epoch 425/500, Loss: 14.4818\n",
            "Epoch 426/500, Loss: 14.8162\n",
            "Epoch 427/500, Loss: 14.8094\n",
            "Epoch 428/500, Loss: 14.4994\n",
            "Epoch 429/500, Loss: 14.9825\n",
            "Epoch 430/500, Loss: 14.6666\n",
            "Epoch 431/500, Loss: 14.7828\n",
            "Epoch 432/500, Loss: 14.7756\n",
            "Epoch 433/500, Loss: 14.9868\n",
            "Epoch 434/500, Loss: 14.8005\n",
            "Epoch 435/500, Loss: 15.2654\n",
            "Epoch 436/500, Loss: 14.5492\n",
            "Epoch 437/500, Loss: 14.4226\n",
            "Epoch 438/500, Loss: 14.8948\n",
            "Epoch 439/500, Loss: 15.1100\n",
            "Epoch 440/500, Loss: 14.8619\n",
            "Epoch 441/500, Loss: 14.9135\n",
            "Epoch 442/500, Loss: 14.7982\n",
            "Epoch 443/500, Loss: 14.5699\n",
            "Epoch 444/500, Loss: 14.7321\n",
            "Epoch 445/500, Loss: 14.7554\n",
            "Epoch 446/500, Loss: 14.5013\n",
            "Epoch 447/500, Loss: 14.7687\n",
            "Epoch 448/500, Loss: 14.5578\n",
            "Epoch 449/500, Loss: 14.7044\n",
            "Epoch 450/500, Loss: 14.8597\n",
            "Epoch 451/500, Loss: 14.1899\n",
            "Epoch 452/500, Loss: 14.8287\n",
            "Epoch 453/500, Loss: 14.5841\n",
            "Epoch 454/500, Loss: 14.8570\n",
            "Epoch 455/500, Loss: 15.0127\n",
            "Epoch 456/500, Loss: 14.9224\n",
            "Epoch 457/500, Loss: 14.9288\n",
            "Epoch 458/500, Loss: 14.8635\n",
            "Epoch 459/500, Loss: 14.7244\n",
            "Epoch 460/500, Loss: 14.4828\n",
            "Epoch 461/500, Loss: 14.3913\n",
            "Epoch 462/500, Loss: 14.9562\n",
            "Epoch 463/500, Loss: 14.4346\n",
            "Epoch 464/500, Loss: 14.7229\n",
            "Epoch 465/500, Loss: 14.5352\n",
            "Epoch 466/500, Loss: 14.5379\n",
            "Epoch 467/500, Loss: 14.4757\n",
            "Epoch 468/500, Loss: 14.4009\n",
            "Epoch 469/500, Loss: 14.2914\n",
            "Epoch 470/500, Loss: 14.1645\n",
            "Epoch 471/500, Loss: 14.1643\n",
            "Epoch 472/500, Loss: 14.4693\n",
            "Epoch 473/500, Loss: 14.3271\n",
            "Epoch 474/500, Loss: 14.6320\n",
            "Epoch 475/500, Loss: 14.5585\n",
            "Epoch 476/500, Loss: 14.3265\n",
            "Epoch 477/500, Loss: 14.3050\n",
            "Epoch 478/500, Loss: 13.8205\n",
            "Epoch 479/500, Loss: 14.1844\n",
            "Epoch 480/500, Loss: 14.6530\n",
            "Epoch 481/500, Loss: 14.5865\n",
            "Epoch 482/500, Loss: 14.1813\n",
            "Epoch 483/500, Loss: 14.1678\n",
            "Epoch 484/500, Loss: 14.2618\n",
            "Epoch 485/500, Loss: 13.9356\n",
            "Epoch 486/500, Loss: 14.1203\n",
            "Epoch 487/500, Loss: 14.2543\n",
            "Epoch 488/500, Loss: 14.2997\n",
            "Epoch 489/500, Loss: 14.1414\n",
            "Epoch 490/500, Loss: 14.1628\n",
            "Epoch 491/500, Loss: 14.1742\n",
            "Epoch 492/500, Loss: 13.9471\n",
            "Epoch 493/500, Loss: 14.0718\n",
            "Epoch 494/500, Loss: 14.4756\n",
            "Epoch 495/500, Loss: 14.5605\n",
            "Epoch 496/500, Loss: 14.1994\n",
            "Epoch 497/500, Loss: 13.8781\n",
            "Epoch 498/500, Loss: 14.3024\n",
            "Epoch 499/500, Loss: 13.8748\n",
            "Epoch 500/500, Loss: 13.9076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to test, just generate more random whole masks, generate context for them, pass only context to network, and see"
      ],
      "metadata": {
        "id": "HrbERaE1DXBd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58JOLMxLEHq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}